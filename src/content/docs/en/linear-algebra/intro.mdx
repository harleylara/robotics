---
title: Introduction
---

import ScalarMult from "../../../../components/LinearAlgebra/ScalarMult.astro";
import AddVector from "../../../../components/LinearAlgebra/AddVector.astro";
import VectorLength from "../../../../components/LinearAlgebra/VectorLength.astro"
import UnitVector from "../../../../components/LinearAlgebra/UnitVector.astro"
import DotProduct from "../../../../components/LinearAlgebra/DotProduct.astro"
import CrossProduct from "../../../../components/LinearAlgebra/CrossProduct.astro"

## Scalar

<Definition term="scalar">
A scalar is a one-component quantity that is invariant under rotations of the coordinate system.

Source: [Scalar - Mathworld Wolfram](https://mathworld.wolfram.com/Scalar.html)

Scalar representation: $a$
</Definition>

## Vector

<Definition term="vector">
In mathematics, physics, and engineering, a Euclidean vector or simply a vector (sometimes called a geometric vector or spatial vector) is a geometric object that has magnitude (or length) and direction.

Vector representation: $\vx = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$

source: [Euclidean vector  - Wikipedia](https://en.wikipedia.org/wiki/Euclidean_vector)

</Definition>

Operations on vectors:
- Scalar multiplication
- Addition and Subtraction
- Length
- Normalized vector
- Dot product
- Cross product

### Scalar Multiplication

<Definition>
    $a \cdot \vv$
    Scalar **bold** multiplication. The scalar $a$ multiply the vector $\vv$
</Definition>

<ScalarMult>
    Original vector.
</ScalarMult>

### Addition and Subtraction

<Definition>
    $\vc + \vd$
    Addition and Subtraction of vectors.
</Definition>

<AddVector>
    Interaction between vectors addition.
</AddVector>

### Length

#### The norm of a vector

<Definition>
    $\Vert \vx \Vert_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$
    On the n-dimensional Euclidean space $\sR^n$ the intuitive notion of length of the vector $\vx = (x_1, x_2, \cdots , x_n)$ is captured by this formula
</Definition>

The Euclidean norm is also called the $L^{2}$ norm, $\ell^{2}$ norm, 2-norm, or square norm; see $L^{p}$ space.

<VectorLength>
    Representation of Euclidean norm.
</VectorLength>

#### The Norm of a Matrix

Just to recap, the triangle inequality says:

<Definition>
    $\Vert \vx + \vy \Vert \leq \Vert \vx \Vert \Vert \vy \Vert$
    Triangle inequality.
</Definition>

This also applies to matrices:

<Definition>
    $\Vert \mA + \mB \Vert \leq \Vert \mA \Vert \Vert \mB \Vert$
    Triangle inequality applied to matrices.
</Definition>

### Normalized Vector

<Definition>
    $\uvx = \frac{\vx}{\Vert \vx \Vert_2}$

    The term *normalized vector* is sometimes used as a synonym for *unit vector*.
</Definition>

<UnitVector>
    Unit vector representation.
</UnitVector>

### Dot Product

<Definition>
    $\va \cdot \vb = \sum_{i=1}^{n} a_{i} b_{i}$
    Coordinate definition of the dot product.
</Definition>

<Definition>
    $\va \cdot \vb = \Vert \va \Vert \cdot \Vert \vb \Vert cos\theta$
    Geometric definition of the dot product. Where $\Vert \va \Vert$ and $\Vert \vb \Vert$ are the euclidean norm and $\theta$ is the angle between the vectors
</Definition>

$$
\vx \cdot \vy = \begin{Vmatrix} \vx \end{Vmatrix} \begin{Vmatrix} \vy \end{Vmatrix} \cos{\theta}
$$

if $\vx \cdot \vy = 0$, $\vx$ and $\vy$ are orthogonal

$$
\cos{\theta} = \frac{\vx \cdot \vy}{\begin{Vmatrix} \vx \end{Vmatrix} \begin{Vmatrix} \vy \end{Vmatrix}}
$$

<DotProduct/>

### Cross Product

<Definition>
    $\va \times \vb = \Vert \va \Vert \Vert \vb \Vert sin(\theta) \uvn$
    where: - $\theta$ is the angle between $\va$ and $\vb$ in the plane containing them $\Vert \va \Vert$ and $\Vert \vb \Vert$ are the magnitudes of vectors $\va$ and $\vb$ and $\uvn$ is a unit vector perpendicular to the plane containing $\va$ and $\vb$, with direction such that the ordered set ($\va$, $\vb$, $\uvn$) is positively-oriented.
</Definition>

<CrossProduct/>

## Matrices

Rectangular array of numbers

$$
\mX = \begin{pmatrix}
x_{11} && x_{12} && \cdots && x_{1m} \\
x_{21} && x_{22} && \cdots && x_{2m} \\
\vdots && \vdots && \ddots && \vdots \\
x_{n1} && x_{n2} && \cdots && x_{nm} \\
\end{pmatrix} \in \sR^{n \times m}
$$

note:
- First index refers to row
- Second index refers to column

### Types of Matrices

- Square matrix
- Diagonal matrix
- Identity matrix $\mI$ or $\mI_n$
- Upper and lower triangular matrix
- Symmetric matrix $\mX = \mX^\transpose$
- Skew-symmetric matrix $\mX = -\mX^\transpose$
- Positive (Semi) definite matrix $\va^\transpose \mX \va \geq 0$
- Orthogonal matrix $\mX^\transpose = \mX^{-1}$

### Operations on Matrices

- Matrix-vector multiplication $\mM \vx$
- Matrix-matrix multiplication $\mM_1 \mM_2$
- Inverse $\mM^{-1}$
- Transpose $\mM^\transpose$

## Tensor


