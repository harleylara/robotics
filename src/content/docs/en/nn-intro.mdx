---
title: Introduction to Neural Networks
describe: Let's see inside a NN
---

export const components = {
    blockquote: Blockquote,
    a: A,
    pre: Pre,
    code: CodeBlock,
    pre: InlineCode,
    em: Em,
    h1: H1,
    h2: H2,
    h3: H3,
    h4: H4,
    h5: H5,
    h6: H6,
    hr: Hr,
    img: Img,
    ul: Ul,
    ol: Ol,
    li: Li,
    strong: Strong,
    p: P
}

## Introduction

- The brain is a highly complex, nonlinear, and parallel computer.

Build up a set of rules and behaviors through was is refer as "experience".

Neuron or processing unit: a single computing cell/unit.


> A neural network is a massively parallel distributed processor made up of simple processing units that has a natural propensity for storing experiential knowledge and making it available for use.

1. Knowledge is acquired by the network from its environment through a learning process.
2. Interneuron connection strengths, known as synaptic weights, are used to store the acquired knowledge.

learning algorithm: is the procedure of modifying the synaptic weights in order to attain a desired objective.

> Generalization refers to the neural network’s production of reasonable outputs for inputs not encountered during training (learning).


NN capabilities:
- Nonlinearity
- Input–Output Mapping
- Adaptivity
- Evidential Response
- Fault Tolerance
- VLSI Implementability (Old fashon)


Human brain:
- approximately 10 billion neurons in the human cortex, and 60 trillion synapses or connections (Shepherd and Koch, 1990).
- the energetic efficiency of the brain is approximately $10^{-16}$ joules (J)


## Rosenblatt’s perceptron

<Important
    note="The perceptron is the simplest form of a neural network used for the classification of patterns said to be **linearly separable** (i.e., patterns that lie on opposite sides of a hyperplane)"
/>

<Drawio
    file="nn-intro.drawio"
    page="1"
    description="Linear and non-linear separability"
/>

<Important
    note="the classes have to be linearly separable for the perceptron to work properly"
/>

<Drawio
    file="nn-intro.drawio"
    page="2"
    description="Perceptron model"
/>

<Equation
    formula="u^{(k)} = \sum_{i = 1}^{m} w^{(k)}_{i} x^{(k)}_{i} + b^{(k)} \\ \\
    y^{(k)} = \varphi(u^{(k)})"
    description="perceptron"
/>

### Perceptron Convergence Theorem

<Drawio
    file="nn-intro.drawio"
    page="3"
    description="Perceptron - The bais term as part of the weights vector"
/>

<Equation
    formula="\vx^{(k)} = [+1, x^{(k)}_{1}, x^{(k)}_{2}, x^{(k)}_{3}, \dots, x^{(k)}_{m}]^\transpose"
    description="input vector"
/>

<Equation
    formula="\vw^{(k)} = [b^{(k)}, w^{(k)}_{1}, w^{(k)}_{2}, w^{(k)}_{3}, \dots, w^{(k)}_{m}]^\transpose"
    description="weights vector"
/>

<Equation
    formula="u^{(k)} = \sum_{i = 0}^{m} w^{(k)}_{i} x^{(k)}_{i} \\
    u^{(k)} = \vw^{\transpose (k)} \vx^{(k)}"
    description="Compact form of the linear combiner"
/>

## Computational Learning Theory

Question: "when if you do well with training data if you will have small amount of our training data you will do well in the test data"

- [Complete Statistical Theory of Learning (Vladimir Vapnik)](https://www.youtube.com/watch?v=Ow25mjFjSmg)
- [Complete statistical theory of learning: learning using statistical invariants](https://proceedings.mlr.press/v128/vapnik20a.html#:~:text=Statistical%20theory%20of%20learning%20considers,called%20strong%20mode%20of%20convergence.)


VC-dimension




