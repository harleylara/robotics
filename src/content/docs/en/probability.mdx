---
title: Fundamentals of Probability
description: Losing and finding my mind
---

## Motivation

**Personal motivation**: I would be one of the first to ask, why some notes on probability in the context of "robotics notes"? my personal answer is because there are simply many fields that extend to the area of robotics that require foundations in probability, to mention some of them; state estimation, deep learning, machine learning, gausing processes, PAC Learning and many more. Therefore, the main purpose of these notes is to lay the necessary foundations in the area of probability in order to be able to explore more areas of robotics later on.

In addition to the motivation to expand my knowlege-base in robotics it is fair to mention the motivations sown by other people, in this case [Prof. Dr. Alexander Asteroth](https://www.h-brs.de/de/inf/prof-dr-alexander-asteroth) who in both his courses "Bayesian Inference and Gaussian Processes" and "Complexity, Randomization, Approximation and PAC-Learning" made me rediscover how fascinating this area is.

Honestly, this note is also driven by a desire to learn and to explore beyond formulas and procedures -- to dive into ideas.

## Historical Background

See: @stiglerHistoryStatisticsMeasurement1986

## About Events

Equivalent terms: Event, experiment, process


<Drawio
    file="probability.drawio"
    page="0"
    description="Types of processes and outcomes."
/>

<Definition
    term="Random Event"
    definition="A random event is the complement of a deterministic event in the sense that if a deterministic event is one whose outcome is completely predictable, by complementarity the outcome of a random event is not fully predictable."
    source="Introduction to Bayesian Scientific Computing: Ten Lectures on Subjective Computing (2007), section 1.1"
/>

I took the following excerpt from the from [Introduction to Bayesian Scientific Computing](https://link.springer.com/book/10.1007/978-0-387-73394-4) and decided to copy it as it is written because I think it is very well put:

*"[...] randomness means lack of information. The degree of information, or more generally, our belief about the outcome of a random event, is expressed in terms of probability."* @calvettiIntroductionBayesianScientific2007 in section 1.1.

*"That said, it is clear that the concept of probability has a **subjective** component, since it is a subjective matter to decide what is reasonable to believe, and what previous experience the belief is based on. The concept of probability advocated in this book is called **subjective probability**, which is often a synonym of **Bayesian probability**"*

export const think_modelling_random = "How can we model a random process or 'experiment' in away that allow us to predict the outcome? For example how can we define the process of throwing a die?"

<Think
    note={think_modelling_random}
/>

The answer to this is provided by the concept of **Probability Space**: is a mathematical construct that provides a formal model of a random process or "experiment". [Probability Space - Wikipedia](https://en.wikipedia.org/wiki/Probability_space)

Let's break it down.

Side track to the rabbit hole:
- [What is Random?](https://www.youtube.com/watch?v=9rIy0xY99a0) by Vsauce
- [what is NOT Random?](https://www.youtube.com/watch?v=sMb00lz-IfE) by Veritasium
- [Computational irreducibility - Wolfram MathWorld](https://mathworld.wolfram.com/ComputationalIrreducibility.html)
    - [Stephen Wolfram: ChatGPT and the Nature of Truth, Reality & Computation | Lex Fridman Podcast #376](https://youtu.be/PdE-waSx-d8?t=737), talking about Computational irreducibility.

## The Building Blocks

The formalization of the probability theory is largely due to [Andrey Kolmogorov](https://en.wikipedia.org/wiki/Andrey_Kolmogorov), in his book @kolmogoroffGrundbegriffeWahrscheinlichkeitsrechnung1933 (later tranlated into @kolmogorovFoundationsTheoryProbability1960), who a as @jacodProbabilityEssentials2004 mention at the beginning of chapter 1, Kolmogorov *"[...] saw the connection between the ideas of Borel and Lebesgue and probability theory and gave probability theory its rigorous basis of measurement theory."*, but *"[...] Paul Lévy set the tone for modern Probability [...]"*. In addition, @jaynesProbabilityTheoryLogic2003 provides a comparative view of Kolmogorov's approach on probability (and others) and constracts it with the ideas derived in his book ("Probability Theory: The logic of science"),

Probability as we know it today has its roots in [Measure Theory](https://en.wikipedia.org/wiki/Measure_(mathematics)). Measure Theory provides a framework for generalizing and formalizing the concept of "measure" (in the most generic extension of the word) to any context. Measures are fundamental to probability theory, the theory of integration and others areas. A good resource to explore is @taoIntroductionMeasureTheory2011. In simple words, measurement theory seeks to assign numbers to "things" by figuring out whether they can actually be measured, and by "things" we mean any kind of things -- physical or abstract --, such as; numbers, symbols, geometric shapes, variables, or even other "collection of things", and I know, the term "things" is not very concrete or formal so in mathematics it is usually referred as **set**.

Side track to the rabbit hole:
- @durrettProbabilityTheoryExamples2019

And what does all this have to do with probability? Well, before explaining the concepts of probability, let me introduce an abstraction tool in term of wording which (hopefully) will help to generalize the ideas and understand the underlaying principles for probability.

In probability the term "events" is used extensibly, so if necessary use the following table to be able to generalize ideas of probability to other domains, or on the contrary to concretize ideas of measure theory in terms of probability.

| Measure Theory          | Probability      |
|-------------------------|------------------|
| Measurable sets         | Events           |
| Element/member of a set | Event            |
| Measurable functions    | Random variables |

From @jacodProbabilityEssentials2004 (Probability Essentials) *"The theory of probability aims towards a mathematical theory -- the probability space -- which describes such phenomena. This theory -- the probability space -- contains three main ingredients"*:
- **Sample space**: $\Omega$
- **Event space**: $\ssA$
- **The probability measure**: $P$

Note: the concept of probability space will be discussed later, but in essence it is derived from the concept of [measure space](https://en.wikipedia.org/wiki/Measure_space) of Measure theory.

At this point we already have a hint (the necessary ingredients) that will help us answer the question: {think_modelling_random}. Now, let's dissect each individual ingredient and its inner workings.

### Sample Space

If when you read the word "space" you think of a physical space — maybe 3D, 2D, 1D, or even N-dimensional — then you're thinking in terms of **Euclidean spaces**, and you're not far from the intuition behind a **sample space**.
And if what first came to your mind was outer space... well, that shows you have a great imagination.

Now, if we push the idea even further and allow for **non-physical spaces**, accepting **abstract spaces**, we open the door to working in much more general settings, such as:
- The space of outcomes when flipping a coin.
- The space of outcomes when rolling a die.

In this way, the concept of "space" becomes a structure that can be as concrete or as abstract as needed to model the situation we want to study. Thus, you can intuitively think of a sample space as the space that lists all possibilities. For example, 
- In a 3D space, the sample space would be all the 3D coordinates — that is, $\sR^3$.
- In the space of flipping a coin, the possibilities are simply: $\{\textrm{heads, tails}\}$.

With this intuition in place we can define:

<Definition
    term="Sample space"
    definition="Denoted $\Omega$, also known as state space, can be any finite or infinite set, meaning that the sample space is a set of **ALL** possible outcomes of an experiment."
/>

For example if you throw a die with 6 faces all the possible outcomes can be represented as:

<Equation
    formula="\Omega = \{1,2,3,4,5,6\}"
    description="Sample space of throwing a 6 faces die."
/>

If the experiment is throwing two (independent) coins the sample space is as follow:

<Equation
    formula="\Omega = \{hh, tt, ht, th\}\text{, where } h \text { is head and } t \text{ is tail}"
    description="Sample space of throwing two (independent) coins."
/>

<Drawio
    file="probability.drawio"
    page="7"
    description="Sample space examples."
/>


### Event Space

With this intuition of sample space, now we can start exploring the space itself and we can try to find "sub-spaces" -- subsets within the defined space -- that we can measure. The problem is that this is not an easy task, for example think the space of all real numbers from zero to one:

$$ \Omega = [0,1] = \{x \in \sR \, \vert \, 0 \leq x \leq 1 \} $$

can you think of "sub-spaces" that can be derived from this space ?

Note: That I'm using the word "sub-spaces" to refer to the more formal concept of **subsets**.

Some examples:
- $\{1,2,3,4\}$ (measurable)
- all the rationals (measurable)
- all the irrationals (measurable)
- cantor set (measurable)
- vitaly set (non-measurable)

As you can see not all of them are measurable, and this is exactly a mathematician named [Giuseppe Vitaly](https://en.wikipedia.org/wiki/Giuseppe_Vitali) found, that not all subsets of a space can be measurable. Luckily for us, in probability we don't have to deal with these unruly sets.

In mathematics there is a way of referring to the collection of all sets in a space, this is called **powerset** ( @taoIntroductionMeasureTheory2011 refer to it as *discrete algebra* ), and is usually denoted as $2^\Omega$ or $\mathcal{P}(\Omega)$, but be careful, because as you can see in our previous example with the set of real numbers, there are spaces in the powerset that may contain subsets that are not measurable, and including them in our analysis would cause us problems. As @michaelbetancourtProbabilityTheoryScientists2018 states: *"[...] we often want to consider not the entire power set but rather a restriction of the power set that avoids any pathological sets that might be hiding."*

But you may be wondering: how do we filter or restrict only those sets that have nice properties and are measurable? What are the properties that determine whether a set is measurable or not?
Well, this is exactly what the Measure Theory branch of mathematics studies, @follandRealAnalysisModern1999 in section 1.2 of the book introduces the concept of **Algebra**, which he then extends to **Sigma-algebra**, this is exactly the solution to our previous question; **Algebras and Sigma-Algebras** define certain properties -- which we will discuss next -- that help us to filter or restrict consistent sets that can be measurable.

<MathDefinition
    concept="Algebra"
    tag="algebra"
    definition={`
Let $\\sX$ be a **non empty set** (our space). A collection $\\ssA$ of subsets of $\\sX$ is a **Algebra** if:
(i). If $\\sE \\in \\ssA$ then $\\scE \\in \\ssA$. In words: Pick some set from $\\sX$, for example $\\sE$, if $\\sE$ is in the algebra its complement is also in the algebra.
(ii). If $\\sE_1, \\dots, \\sE_{n} \\in \\ssA$ then $\\bigcup_{j=1}^{n} \\sE_j \\in \\ssA$. In words: If a **finite** collection of sets ($\\sE_1, \\dots \\sE_{n}$) is in $\\ssA$ then the union of all of them is also in $\\ssA$.
`
}
/>

<Drawio
    file="probability.drawio"
    page="3"
    description="Illustration of an Algebra."
/>

From these two simple but powerful properties of Algebras we can derive more properties:
- $\Omega \in \ssA$ and $\emptyset \in \ssA$. If we place our complete space, in this case demonimated $\Omega$, the first property of Algebras (closed under complements) tells us that we also have to include its complement.
- If $\sE \in \ssA$ then $\scE \in \ssA$. Property (i) by itself: Closed under complements.
- If $\sE_1, \dots, \sE_{n} \in \ssA$ then $\bigcup_{j=1}^{n} \sE_j \in \ssA$. Property (ii) by itself: Closed under **finite** unions.
- If $\sE_1, \dots, \sE_{n} \in \ssA$ then $\bigcap_{j=1}^{n} \sE_j \in \ssA$. Closed under **finite** intersections. This property may not be obvious at first sight, but it is simply a consequence of (i) and (ii).

<Definition
    term="Finite set"
    definition="In mathematics, particularly set theory, a finite set is a set that has a finite number of elements.. Example the set of all outcomes of throwing a dice ${1,2,3,4,5,6}$."
    source="Finite set - Wikipedia"
    link="https://en.wikipedia.org/wiki/Finite_set"
/>

Derivation of **closed under finite intersections**:

You can simply invoke De Morgan law:  $\sA \cap \sB = (\scA \cup \scB)^{\complement}$, so that property (i) and (ii) cause property of closed under intersections. Here is a visual illustration of De Morgan law.

<Drawio
    file="probability.drawio"
    page="9"
    description="Illustration of De Morgan law."
/>

To keep this brief, as an exercise, you can generalize this rule for $n$-sets.

<Think
    note="Why are we doing all this, I thought we were going to talk about Event Space?"
/>

I don't blame you for thinking that, but trust me — we'll get there soon, and hopefully, you'll have an Aha! moment when we introduce the Event Space. Think of this paragraph as a quick stop at the shore to catch your breath before we head back into the water.

Before we get back just a quick recap:
- An abstract space (like the sample space) ...
- can contain subsets that are not measurable ...
- so we can filter or restrict them with the properties of the Algebras

At this point we have made the assumption that we are working with **FINITE unions**, the problem is that in Probability it is not enough, and sometimes we have to work with **Countable unions**. As @taoIntroductionMeasureTheory2011 explains in section 1.4.2, this property of infinite unions is also important in integration theory, and @follandRealAnalysisModern1999 in section 1.1 explains more details about it. Then, to work with **Countable unions** we must use not Algebras, but [**Sigma-algebras**](https://en.wikipedia.org/wiki/%CE%A3-algebra).


If you're thinking, "wait, another concept?" ... yes, another concept but don't worry it will be easy, I promise.

<MathDefinition
    concept="Sigma-algebra"
    tag="sigma-algebra"
    definition={`A $\\sigma$-algebra, also known as $\\sigma$-field or borel-field, **is an algebra** that is closed under countable unions.`}
/>

You may be thinking, "Wait, that's it?". Actually yes, a $\sigma$-algebra is an extension of Algebras that allows us to work with countable unions, so all the properties you learned from an Algebras are present on a $\sigma$-algebra. But in favor of completness, here is an expansion of the $\sigma$-algebra definition.

<MathDefinition
    concept="Sigma-Algebra (explicit)"
    definition={`
We assume a $\\sigma$-algebra denoted $\\ssA$ if it satisfies the following properties:
(i). If $\\sE \\in \\ssA$ then $\\scE \\in \\ssA$. In words: The same property (i) of Algebras.
(ii). If $\\sE_i \\in \\ssA, \\forall i \\in \\sN$, then $\\bigcup_{i=1}^{\\infty} \\sE_i \\in \\ssA$. In words: If a **countable** collection of sets is in $\\ssA$ then the union of all of them is also in $\\ssA$.
`
}
/>

Just as properties (i) and (ii) of an [Algebra](#algebra) imply closure under finite intersections, properties (i) and (ii) of a sigma-algebra imply closure under countable intersections. Keep in mind that for any finite collection of sets $\sA_1, \dots, \sA_n \in \ssA$ the union $\bigcup_{i=1}^{n} \sA_i$ is just a particular case of the countable union, Therefore the properties (i) and (ii) of a $\sigma$-algebra implies finite unions, hence any $\sigma$-algebra is a algebra but there are algebras that are not $\sigma$-algebras.

In simple mathematical words:
- A algebra is **closed** under **finite** unions and complements.
- A Sigma-algebra is **closed** under **countable** unions and complements.

Note: The word "closed" is like a safety belt, which ensures that we can do operations such as complements, unions or interceptions without leaving our space (our set). Why? consistency... But why?  this is out of the scope of probability and these notes so I recommend you to see; @taoIntroductionMeasureTheory2011 and @follandRealAnalysisModern1999.

<Definition
    term='Countable set'
    definition="In mathematics, a set is countable if either it is finite or it can be made in one to one correspondence with the set of natural numbers ([A bijection, bijective function, or one-to-one correspondence](https://en.wikipedia.org/wiki/Bijection))."
    source="Countable set - wikipedia"
    link="https://en.wikipedia.org/wiki/Countable_set"
/>

So,
- the smallest Sigma-algebra ( @taoIntroductionMeasureTheory2011 calls it the *trivial algebra*) is: $\{\emptyset, \sX\}$, also denoted as $\sigma(\sX)$
- the largest Sigma-algebra is: the power set $2^\Omega$

|               | Algebra                     | Sigma-algebra                  |
|---------------|-----------------------------|--------------------------------|
| Unions        | finite only                 | countable (finite or infinite) |
| Complements   | yes                         | yes                            |
| Intersections | yes (using De Morgan's law) | yes (using De Morgan's law)    |




Side track to the rabbit hole:
- [The Man Who Almost Broke Math (And Himself...)](https://www.youtube.com/watch?v=_cr46G2K5Fo), about [Georg Cantor](https://en.wikipedia.org/wiki/Georg_Cantor) and the historical background that laid the foundations of set theory, including explanation of the [Vitali set](https://en.wikipedia.org/wiki/Vitali_set), [timestamp to Vitali set](https://youtu.be/_cr46G2K5Fo?t=1069).


<Drawio
    file="probability.drawio"
    page="8"
    description="Power set of the sample space."
/>

**Event space** $\ssA$, also known just as "events": An event space is a set that contains ALL possible events for a given experiment, [source](https://www.statisticshowto.com/event-space-probability/). (the set of all the subsets of $\Omega$)

The event space is based on the concept of $\sigma$-algebra -- a fundamental concept in Measure Theory --.

The event space $\ssA$ is an algebra if it satisfies properties 1, 2 and 3 above. It is a $\sigma$-algebra, (or a $\sigma$-field) if it satisfies 1, 2, and 4 above. Closure under countable unions directly implies closure under finite unions because a finite union is a special case of countable union. Specially:



### The Probability

**The probability** $P$


## Random Variable

<Equation
    formula="X: \Omega \rightarrow \ssT"
    description="Definition of a random variable"
/>

## Discrete and Continuous

## Law of large numbers

Chapter 2: @durrettProbabilityTheoryExamples2019, "Measure theory ends and probability begins with the definition of independence."

## Jungle of Notation

| Reference                                                        | Set | Complement | Set of sets   |
|------------------------------------------------------------------|-----|------------|---------------|
| @grinsteadGrinsteadSnellsIntroduction2009                        | $A$ | $\tilde A$ | ?             |
| @jacodProbabilityEssentials2004                                  | $A$ | $A^{c}$    | $\mathcal{A}$ |
| @jaynesProbabilityTheoryLogic2003                                | $A$ | $\bar{A}$  | $F$           |
| @rasmussenGaussianProcessesMachine2008                           | $A$ | $A^{c}$    | $\mathcal{A}$ |
| @walpoleProbabilityStatisticsEngineers2017                       | $A$ | $A'$       | $\mathcal{A}$ |
| @deisenrothMathematicsMachineLearning (chapter 6)                | $A$ | ?          | $\mathcal{A}$ |
| @bishopPatternRecognitionMachine2006 (chapter 1.2 and chapter 2) | $A$ | TBA        | TBA           |
| @bishopDeepLearningFoundations2024 (chapter 2)                   | $A$ | TBA        | TBA           |
| @goodfellowDeepLearning2016 (chapter 3)                          | $A$ | TBA        | TBA           |
 

## External Resources

- [Probability Essentials by Jean Jacod, Philip Protter](https://link.springer.com/book/10.1007/978-3-642-55682-1) chapter 2.

## References
