---
title: The Least-Mean-Square Algorithm
description: LMS in a way that you may not have seen it before.
---


## Original Paper

- [Adaptative Switching Circuits](https://www-isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf) by Bernard Widrow and Marcian E. Hoff

<Important>
    Not to be confused with Mean Square Error (MSE).
</Important>

## Whats Makes LMS Cool?

- Complexity is linear
- Simple
- Roboust

## The Filtering Nature of LMS

Let's Explore a Dynamic Linear System:

<Drawio file="lms.drawio" page="1">
    A simple dynamic linear system.
</Drawio>

<Equation>
    $$
    \mathcal{T} = \{\vx(t), d(t) ; i = 1,2,\dots, n\}
    $$
    Data set for the simple dynamic linear system.
</Equation>

<Equation>
    $$
    \vx(t) = x_{1}(t), x_{2}(t), \dots, x_{M}(t)
    $$

    Input vector $\vx$ at a given time $t$ also called **stimulus vector**.
</Equation>

Harley: Ok wait a second, Does the stimulus vector represents all the **current**/**present** input values at a given time $t$ or it also include **pass** input values?

In fact there are two way to represent the **stimulus vector**:
- **spatial** all the values originate at different points in space, like a *snapshot of data*.
- **temporal** this includes **present** but also **pass** ($M-1$) values of some excitation that are uniformly spaced in time.


TBD: discuss about adaptive filter and the relationship with LMS

<Equation>
  $$
  y(t) = v(t) = \sum_{k=1}^{M}w_{k}(t)x_{k}(t)
  $$

  The output value of a linear neuron.
</Equation>

This can be represented as the **inner product**


<Equation>
    $$
    y(t) = v(t) = \vx^{\transpose}(t)\vw(t)
    $$
    The output value of a linear neuron.
</Equation>

Where:

<Equation>
    $$
    \vw(t) = [w_{1}(t), w_{2}(t), \dots, w_{3}(t)]^{\transpose}
    $$
    Synaptic weight vector $\vw$ at time $t$ for a single neuron.
</Equation>

Don't forget:

<Equation>
    $$
    e(t) = d(t) - y(t)
    $$

    Error signal.
</Equation>

<Important>
    The manner in which the error signal e(i) is used to control the adjustments to the neuron’s synaptic weights is determined by the cost function used to derive the adaptivefiltering algorithm of interest.
</Important>

## Unconstrained Optimization: a review

Unconstrained optimization:
- Steepest Descent
- Newton’s Method
- Gauss–Newton Method

Consider a cost function (also called loss function) $\mathcal{L}(\vw)$ that is a **continuously differentiable** function of some unknown weight (parameter) vector $\vw$.

More information about [differentiable functions](https://en.wikipedia.org/wiki/Differentiable_function).

<Equation>
    $$
    \mathcal{L}(\vw^{*}) \leq \mathcal{L}(\vw)
    $$
</Equation>

<Important>
    Minimize the cost function $\mathcal{L}$ with respect to the weight vector $\vw$.
</Important>

The necessary condition for optimality is:

<Equation>
    $$
    \nabla\mathcal{L}(\vw^{*}) = \mathbf{0}
    $$
    Condition for optimality.
</Equation>

Where $\nabla$ is the **gradient operator**,

<Equation>
    $$
    \nabla = \begin{bmatrix}
    \frac{\partial}{\partial w_{1}}, \frac{\partial}{\partial w_{2}}, \dots, \frac{\partial}{\partial w_{M}}
\end{bmatrix}^{\transpose}
    $$
    Gradient operator.
</Equation>

and $\nabla \mathcal{L}(\vw)$ is the **gradient vector** of the cost function:

<Equation>
    $$
    \nabla \mathcal{L}(\vw) = \begin{bmatrix}
    \frac{\partial \mathcal{L}}{\partial w_{1}},
    \frac{\partial \mathcal{L}}{\partial w_{2}}, 
    \dots, 
    \frac{\partial \mathcal{L}}{\partial w_{M}}
    \end{bmatrix}^{\transpose}
    $$
    Gradient operator.
</Equation>

Starting with an initial guess denoted by $\vw(0)$, generate a sequence of weight vectors $\vw(1)$, $\vw(2)$, $...$, such that the cost function $\mathcal{L}(\vw)$ is reduced at each iteration of the algorithm, as shown by:

<Equation>
    $$
    \mathcal{L}(\vw(i + 1)) < \mathcal{L}(\vw(i))
    $$
</Equation>

where $\vw(i)$ is the old value of the weight vector and $\vw(i + 1)$ is its updated value.

<Important>
    We **hope** that the algorithm will eventually converge onto the optimal solution $\vw^{*}$.
</Important>

The term *"hope"* is due to the possibility that the algorithm will diverge

Some references:
- [Dynamic programming and optimal control, 3rd Edition](https://www.semanticscholar.org/paper/Dynamic-programming-and-optimal-control%2C-3rd-Bertsekas/a82db864e472b5aa6313596ef9919f64e3363b1f) by D. Bertsekas 1995
- [Constrained vs Unconstrained Formulations](https://qiml.radiology.wisc.edu/wp-content/uploads/sites/760/2021/03/lecture_A1_03_Constraints.pdf)

### Gradient Descent

(also often called steepest descent)

For convenience representation:

<Equation>
    $$
    \vg = \nabla \mathcal{L}(\vw)
    $$
    Gradient of the weight vector $\vw$.
</Equation>

<Equation>
    $$
    \vw(i + 1) = \vw(i) + \eta \nabla \mathcal{L}(\vw)
    $$
    Formal defintion of the gradient descent. Where $\eta$ is a positive constant called the *stepsize*, or *learning-rate*.
</Equation>

using the convenient representation shown above

<Equation>
    $$
    \vw(i + 1) = \vw(i) + \eta \vg(i)
    $$
</Equation>

the learning-rate parameter  has a profound influence on its convergence behavior:the learning-rate parameter  has a profound influence on its convergence behavior:

- When $\eta$ is small, the transient response of the algorithm is *overdamped*
- When $\eta$ is large, the transient response of the algorithm is *underdamped*
- When $\eta$ exceeds a certain critical value, the algorithm becomes unstable (i.e., it diverges).

[Gradient Descent - Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)


