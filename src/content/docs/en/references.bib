@preprint{bhoiMonocularDepthEstimation2019,
  extra = {arXiv:1901.09402 [cs]},
  author = {Bhoi, Amlaan},
  abstractNote = {Monocular depth estimation is often described as an illposed and inherently ambiguous problem. Estimating depth from 2D images is a crucial step in scene reconstruction, 3D object recognition, segmentation, and detection. The problem can be framed as: given a single RGB image as input, predict a dense depth map for each pixel. This problem is worsened by the fact that most scenes have large texture and structural variations, object occlusions, and rich geometric detailing. All these factors contribute to difﬁculty in accurate depth estimation. In this paper, we review ﬁve papers that attempt to solve the depth estimation problem with various techniques including supervised, weakly-supervised, and unsupervised learning techniques. We then compare these papers and understand the improvements made over one another. Finally, we explore potential improvements that can aid to better solve this problem.},
  key = {8TVFATMW},
  libraryCatalog = {arXiv.org},
  title = {Monocular Depth Estimation: A Survey},
  year = {2019},
  language = {en},
  url = {http://arxiv.org/abs/1901.09402},
  date = {2019-01-27 2019-01-27},
  shortTitle = {Monocular Depth Estimation},
  repository = {arXiv},
  archiveID = {arXiv:1901.09402},
  accessDate = {2024-05-28 15:00:24},
}
@journalArticle{padkanEVALUATINGMONOCULARDEPTH2023,
  publicationTitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  journalAbbreviation = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.},
  abstractNote = {Depth estimation from monocular images has become a prominent focus in photogrammetry and computer vision research. Monocular Depth Estimation (MDE), which involves determining depth from a single RGB image, offers numerous advantages, including applications in simultaneous localization and mapping (SLAM), scene comprehension, 3D modeling, robotics, and autonomous driving. Depth information retrieval becomes especially crucial in situations where other sources like stereo images, optical flow, or point clouds are not available. In contrast to traditional stereo or multi-view methods, MDE techniques require fewer computational resources and smaller datasets. This research work presents a comprehensive analysis and evaluation of some state-of-the-art MDE methods, considering their ability to infer depth information in terrestrial images. The evaluation includes quantitative assessments using ground truth data, including 3D analyses and inference time.},
  ISSN = {2194-9034},
  author = {Padkan, N. and Trybala, P. and Battisti, R. and Remondino, F. and Bergeret, C.},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {137-144},
  key = {7LEQVMK2},
  title = {EVALUATING MONOCULAR DEPTH ESTIMATION METHODS},
  year = {2023},
  language = {en},
  url = {https://isprs-archives.copernicus.org/articles/XLVIII-1-W3-2023/137/2023/},
  rights = {https://creativecommons.org/licenses/by/4.0/},
  accessDate = {2024-06-27 02:01:56},
  date = {2023-10-19 2023-10-19},
  volume = {XLVIII-1/W3-2023},
  DOI = {10.5194/isprs-archives-XLVIII-1-W3-2023-137-2023},
}
@preprint{alhashimHighQualityMonocular2019,
  language = {en},
  accessDate = {2024-10-02 04:07:58},
  title = {High Quality Monocular Depth Estimation via Transfer Learning},
  extra = {arXiv:1812.11941 [cs]},
  archiveID = {arXiv:1812.11941},
  year = {2019},
  date = {2019-03-10 2019-03-10},
  url = {http://arxiv.org/abs/1812.11941},
  author = {Alhashim, Ibraheem and Wonka, Peter},
  abstractNote = {Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction. Existing solutions for depth estimation often produce blurry approximations of low resolution. This paper presents a convolutional neural network for computing a high-resolution depth map given a single RGB image with the help of transfer learning. Following a standard encoder-decoder architecture, we leverage features extracted using high performing pre-trained networks when initializing our encoder along with augmentation and training strategies that lead to more accurate results. We show how, even for a very simple decoder, our method is able to achieve detailed high-resolution depth maps. Our network, with fewer parameters and training iterations, outperforms state-of-the-art on two datasets and also produces qualitatively better results that capture object boundaries more faithfully. Code and corresponding pre-trained weights are made publicly available1.},
  libraryCatalog = {arXiv.org},
  repository = {arXiv},
  key = {32BVAV9H},
}
@conferencePaper{BlochLaurent2015,
  title = {Informatics in the light of some Leibniz’s works},
  year = {2016},
  date = {2016-05-00 2016-05},
  author = {Bloch, Laurent},
  key = {3YCK8RDT},
  extra = {Citation Key: BlochLaurent2015},
}
@preprint{tzesGraphNeuralNetworks2022,
  title = {Graph Neural Networks for Multi-Robot Active Information Acquisition},
  abstractNote = {This paper addresses the Multi-Robot Active Information Acquisition (AIA) problem, where a team of mobile robots, communicating through an underlying graph, estimates a hidden state expressing a phenomenon of interest. Applications like target tracking, coverage and SLAM can be expressed in this framework. Existing approaches, though, are either not scalable, unable to handle dynamic phenomena or not robust to changes in the communication graph. To counter these shortcomings, we propose an Information-aware Graph Block Network (I-GBNet), an AIA adaptation of Graph Neural Networks, that aggregates information over the graph representation and provides sequential-decision making in a distributed manner. The I-GBNet, trained via imitation learning with a centralized sampling-based expert solver, exhibits permutation equivariance and time invariance, while harnessing the superior scalability, robustness and generalizability to previously unseen environments and robot conﬁgurations. Experiments on signiﬁcantly larger graphs and dimensionality of the hidden state and more complex environments than those seen in training validate the properties of the proposed architecture and its efﬁcacy in the application of localization and tracking of dynamic targets.},
  archiveID = {arXiv:2209.12091},
  key = {UETYTIWH},
  extra = {arXiv:2209.12091 [cs]},
  year = {2022},
  url = {http://arxiv.org/abs/2209.12091},
  repository = {arXiv},
  language = {en},
  accessDate = {2023-08-15 11:31:30},
  author = {Tzes, Mariliza and Bousias, Nikolaos and Chatzipantazis, Evangelos and Pappas, George J.},
  libraryCatalog = {arXiv.org},
  date = {2022-09-24 2022-09-24},
}
@preprint{bochkovskiiDepthProSharp2024,
  language = {en},
  accessDate = {2024-11-11 15:51:14},
  author = {Bochkovskii, Aleksei and Delaunoy, Amaël and Germain, Hugo and Santos, Marcel and Zhou, Yichao and Richter, Stephan R. and Koltun, Vladlen},
  repository = {arXiv},
  extra = {arXiv:2410.02073 [cs]},
  abstractNote = {We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro},
  title = {Depth Pro: Sharp Monocular Metric Depth in Less Than a Second},
  libraryCatalog = {arXiv.org},
  url = {http://arxiv.org/abs/2410.02073},
  shortTitle = {Depth Pro},
  key = {JJ5C6VKZ},
  date = {2024-10-02 2024-10-02},
  year = {2024},
  archiveID = {arXiv:2410.02073},
}
@preprint{eigenDepthMapPrediction2014,
  year = {2014},
  archiveID = {arXiv:1406.2283},
  title = {Depth Map Prediction from a Single Image using a Multi-Scale Deep Network},
  author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
  abstractNote = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence sufﬁces for estimation, ﬁnding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that reﬁnes this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
  libraryCatalog = {arXiv.org},
  extra = {arXiv:1406.2283 [cs]},
  key = {TCZ78FYN},
  repository = {arXiv},
  accessDate = {2023-08-10 12:19:39},
  language = {en},
  url = {http://arxiv.org/abs/1406.2283},
  date = {2014-06-09 2014-06-09},
}
@journalArticle{zhaoMonocularDepthEstimation2020,
  libraryCatalog = {arXiv.org},
  pages = {1612-1627},
  author = {Zhao, Chaoqiang and Sun, Qiyu and Zhang, Chongzhen and Tang, Yang and Qian, Feng},
  extra = {arXiv:2003.06620 [cs]},
  url = {http://arxiv.org/abs/2003.06620},
  publicationTitle = {Science China Technological Sciences},
  language = {en},
  abstractNote = {Depth information is important for autonomous systems to perceive environments and estimate their own state. Traditional depth estimation methods, like structure from motion and stereo vision matching, are built on feature correspondences of multiple viewpoints. Meanwhile, the predicted depth maps are sparse. Inferring depth information from a single image (monocular depth estimation) is an ill-posed problem. With the rapid development of deep neural networks, monocular depth estimation based on deep learning has been widely studied recently and achieved promising performance in accuracy. Meanwhile, dense depth maps are estimated from single images by deep neural networks in an end-to-end manner. In order to improve the accuracy of depth estimation, different kinds of network frameworks, loss functions and training strategies are proposed subsequently. Therefore, we survey the current monocular depth estimation methods based on deep learning in this review. Initially, we conclude several widely used datasets and evaluation indicators in deep learning-based depth estimation. Furthermore, we review some representative existing methods according to different training manners: supervised, unsupervised and semisupervised. Finally, we discuss the challenges and provide some ideas for future researches in monocular depth estimation.},
  accessDate = {2024-06-14 06:36:43},
  ISSN = {1674-7321, 1869-1900},
  volume = {63},
  journalAbbreviation = {Sci. China Technol. Sci.},
  key = {5IWGDN7U},
  date = {2020-09-00 09-2020},
  DOI = {10.1007/s11431-020-1582-8},
  issue = {9},
  title = {Monocular Depth Estimation Based On Deep Learning: An Overview},
  year = {2020},
  shortTitle = {Monocular Depth Estimation Based On Deep Learning},
}
@conferencePaper{vandijkHowNeuralNetworks2019,
  rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  author = {Van Dijk, Tom and De Croon, Guido},
  conferenceName = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  ISBN = {978-1-7281-4803-8},
  abstractNote = {Deep neural networks have lead to a breakthrough in depth estimation from single images. Recent work shows that the quality of these estimations is rapidly increasing. It is clear that neural networks can see depth in single images. However, to the best of our knowledge, no work currently exists that analyzes what these networks have learned.},
  url = {https://ieeexplore.ieee.org/document/9009532/},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {2183-2191},
  proceedingsTitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  language = {en},
  place = {Seoul, Korea (South)},
  date = {2019-10-00 2010-10-00},
  key = {XMZVKHSM},
  title = {How Do Neural Networks See Depth in Single Images?},
  publisher = {IEEE},
  DOI = {10.1109/ICCV.2019.00227},
  year = {2019},
  accessDate = {2025-01-04 22:19:24},
}
@journalArticle{khaldiOverviewSwarmRobotics2015,
  date = {2015-09-17 2015-09-17},
  publicationTitle = {International Journal of Computer Applications},
  issue = {2},
  journalAbbreviation = {IJCA},
  author = {Khaldi, Belkacem and Cherif, Foudil},
  language = {en},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {31-37},
  url = {http://www.ijcaonline.org/research/volume126/number2/khaldi-2015-ijca-906000.pdf},
  key = {N2A9WKS6},
  year = {2015},
  abstractNote = {As an emergent research area by which swarm intelligence is applied to multi-robot systems; swarm robotics (a very particular and peculiar sub-area of collective robotics) studies how to coordinate large groups of relatively simple robots through the use of local rules. It focuses on studying the design of large amount of relatively simple robots, their physical bodies and their controlling behaviors. Since its introduction in 2000, several successful experimentations had been realized, and till now more projects are under investigations. This paper seeks to give an overview of this domain research; for the aim to orientate the readers, especially those who are newly coming to this research field.},
  ISSN = {09758887},
  DOI = {10.5120/ijca2015906000},
  title = {An Overview of Swarm Robotics: Swarm Intelligence Applied to Multi-robotics},
  shortTitle = {An Overview of Swarm Robotics},
  accessDate = {2025-02-03 15:31:06},
  volume = {126},
}
@journalArticle{saxenaLearningDepthSingle,
  key = {J3QQAXIN},
  title = {Learning Depth from Single Monocular Images},
  year = {NA},
  abstractNote = {We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufﬁcient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.},
  author = {Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y},
  language = {en},
  libraryCatalog = {Zotero},
}
@journalArticle{tanSwarmRoboticsCollective2013,
  key = {GLFLXVWD},
  title = {Swarm Robotics: Collective Behavior Inspired by Nature},
  url = {https://www.hilarispublisher.com/open-access/swarm-robotics-collective-behavior-inspired-by-nature-jcsb.1000e106.pdf},
  DOI = {10.4172/jcsb.1000e106},
  volume = {6},
  year = {2013},
  ISSN = {0974-7230},
  publicationTitle = {Journal of Computer Science & Systems Biology},
  date = {2013-00-00 2013},
  author = {Tan, Ying},
  language = {en},
  libraryCatalog = {Zotero},
}
@journalArticle{sharkeySwarmRoboticsMinimalism2007,
  key = {7U4USFRL},
  title = {Swarm robotics and minimalism},
  DOI = {10.1080/09540090701584970},
  year = {2007},
  pages = {245-260},
  issue = {3},
  libraryCatalog = {DOI.org (Crossref)},
  date = {2007-09-00 2007-09},
  url = {https://www.tandfonline.com/doi/full/10.1080/09540090701584970},
  journalAbbreviation = {Connection Science},
  volume = {19},
  author = {Sharkey, Amanda J. C.},
  language = {en},
  ISSN = {0954-0091, 1360-0494},
  publicationTitle = {Connection Science},
  accessDate = {2025-02-04 02:13:30},
}
@bookSection{beniSwarmIntelligenceCellular1993,
  key = {22EJZYDL},
  title = {Swarm Intelligence in Cellular Robotic Systems},
  extra = {DOI: 10.1007/978-3-642-58069-7_38},
  place = {Berlin, Heidelberg},
  year = {1993},
  pages = {703-712},
  abstractNote = {Cellular Robotic Systems are capable of 'intelligent' behavior. The meaning of this intelligence is analyzed in the paper. We define robot intelligence and robot system intelligence in terms of unpredictability of improbable behavior. The concept of unpredictability is analyzed in relation to (1) statistical unpredictability, (2) inaccessibility, (3) undecidability, (4) intractability, and (5) non-representability. We argue that the latter two type of unpredictability, when exhibited by systems capable of producing order, can result in a non-trivial, different form of intelligent behavior (Swarm Intelligence). Engineering problems related to Swarm Intelligence are mentioned in relation to Cellular Robotic Systems which consist of collections of autonomous, non-synchronized, non-. intelligent robots cooperating to achieve global tasks.},
  publisher = {Springer Berlin Heidelberg},
  rights = {http://www.springer.com/tdm},
  libraryCatalog = {DOI.org (Crossref)},
  date = {1993-00-00 1993},
  url = {http://link.springer.com/10.1007/978-3-642-58069-7_38},
  bookTitle = {Robots and Biological Systems: Towards a New Bionics?},
  author = {Dario, Paolo and Sandini, Giulio and Aebischer, Patrick and Beni, Gerardo and Wang, Jing},
  ISBN = {978-3-642-63461-1 978-3-642-58069-7},
  language = {en},
  accessDate = {2025-02-04 02:24:35},
}
@thesis{ferranteControlArchitectureHeterogeneous2009,
  key = {YR4DAQKC},
  title = {A Control Architecture for a Heterogeneous Swarm of Robots},
  url = {https://iridia.ulb.ac.be/~mdorigo/HomePageDorigo/thesis/dea/FerranteMAS.pdf},
  year = {2009},
  thesisType = {PhD Thesis},
  date = {2009-00-00 2009},
  university = {UNIVERSIT ´E LIBRE DE BRUXELLES},
  abstractNote = {We propose a software architecture that can ease and speed up the development process of controllers for heterogeneous swarm systems. It is inherently modular and allows behaviors to be combined in layers and reused in multiple controllers. This can potentially speedup the development process of controllers since they become much more readable and well structured. We validated the architecture through an experiment of collective navigation and obstacle avoidance using two different types of robots. The experiments show that, using the proposed architecture, complex behaviors can indeed be built through the combination of very simple behaviors.},
  author = {Ferrante, Eliseo},
  language = {en},
  libraryCatalog = {Zotero},
}
@bookSection{yangSwarmBasedMetaheuristicAlgorithms2012,
  year = {2012},
  key = {XQTTDU6G},
  title = {Swarm-Based Metaheuristic Algorithms and No-Free-Lunch Theorems},
  url = {http://www.intechopen.com/books/theory-and-new-applications-of-swarm-intelligence/swarm-based-metaheuristic-algorithms-and-no-free-lunch-theorems},
  bookTitle = {Theory and New Applications of Swarm Intelligence},
  ISBN = {978-953-51-0364-6},
  publisher = {InTech},
  extra = {DOI: 10.5772/30852},
  date = {2012-03-16 2012-03-16},
  author = {Parpinelli, Rafael and Yang, Xin-She},
  language = {en},
  libraryCatalog = {DOI.org (Crossref)},
  accessDate = {2025-02-04 16:13:53},
}
@journalArticle{neshatArtificialFishSwarm2014,
  key = {KT57PTXC},
  title = {Artificial fish swarm algorithm: a survey of the state-of-the-art, hybridization, combinatorial and indicative applications},
  DOI = {10.1007/s10462-012-9342-2},
  year = {2014},
  pages = {965-997},
  abstractNote = {AFSA (artiﬁcial ﬁsh-swarm algorithm) is one of the best methods of optimization among the swarm intelligence algorithms. This algorithm is inspired by the collective movement of the ﬁsh and their various social behaviors. Based on a series of instinctive behaviors, the ﬁsh always try to maintain their colonies and accordingly demonstrate intelligent behaviors. Searching for food, immigration and dealing with dangers all happen in a social form and interactions between all ﬁsh in a group will result in an intelligent social behavior. This algorithm has many advantages including high convergence speed, ﬂexibility, fault tolerance and high accuracy. This paper is a review of AFSA algorithm and describes the evolution of this algorithm along with all improvements, its combination with various methods as well as its applications. There are many optimization methods which have a afﬁnity with this method and the result of this combination will improve the performance of this method. Its disadvantages include high time complexity, lack of balance between global and local search, in addition to lack of beneﬁting from the experiences of group members for the next movements.},
  issue = {4},
  rights = {http://www.springer.com/tdm},
  libraryCatalog = {DOI.org (Crossref)},
  date = {2014-12-00 2014-12},
  url = {http://link.springer.com/10.1007/s10462-012-9342-2},
  shortTitle = {Artificial fish swarm algorithm},
  journalAbbreviation = {Artif Intell Rev},
  publicationTitle = {Artificial Intelligence Review},
  author = {Neshat, Mehdi and Sepidnam, Ghodrat and Sargolzaei, Mehdi and Toosi, Adel Najaran},
  language = {en},
  ISSN = {0269-2821, 1573-7462},
  volume = {42},
  accessDate = {2025-02-04 16:17:54},
}
@journalArticle{hamdiBeebasedAlgorithmsReview2010,
  year = {2010},
  date = {2010-10-00 10-2010},
  language = {en},
  libraryCatalog = {Zotero},
  pages = {2},
  title = {Bee-based algorithms: a review},
  key = {UA542PLJ},
  author = {Hamdi, A and Monmarché, N and Alimi, M Adel and Slimane, M},
  url = {https://www.researchgate.net/profile/Amira-Hamdi/publication/237844606_META10_Bee-based_algorithms_a_review_META_2010/links/0046351be9feb2e247000000/META10-Bee-based-algorithms-a-review-META-2010.pdf},
}
@book{sahinSwarmRoboticsSAB2005,
  seriesNumber = {3342},
  author = {Şahin, Erol and Spears, William M.},
  series = {Lecture notes in computer science, State-of-the-art survey},
  publisher = {Springer},
  ISBN = {978-3-540-24296-3},
  shortTitle = {Swarm robotics},
  language = {en},
  libraryCatalog = {Library of Congress ISBN},
  year = {2005},
  numPages = {174},
  key = {HNUULXXY},
  extra = {Meeting Name: International Conference on Simulation of Adaptive Behavior
OCLC: ocm57597119},
  callNumber = {TJ210.3 .I633 2004},
  date = {2005-00-00 2005},
  place = {Berlin ; New York},
  title = {Swarm robotics: SAB 2004 international workshop, Santa Monica, CA, USA, July 17, 2004: revised selected papers},
}
@journalArticle{dorigoEvolvingSelfOrganizingBehaviors2004,
  url = {http://link.springer.com/10.1023/B:AURO.0000033973.24945.f3},
  author = {Dorigo, Marco and Trianni, Vito and Şahin, Erol and Groß, Roderich and Labella, Thomas H. and Baldassarre, Gianluca and Nolfi, Stefano and Deneubourg, Jean-Louis and Mondada, Francesco and Floreano, Dario and Gambardella, Luca M.},
  accessDate = {2025-02-04 20:55:06},
  key = {XDTAB9H8},
  volume = {17},
  publicationTitle = {Autonomous Robots},
  issue = {2/3},
  ISSN = {0929-5593},
  language = {en},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {223-245},
  DOI = {10.1023/B:AURO.0000033973.24945.f3},
  abstractNote = {In this paper, we introduce a self-assembling and self-organizing artifact, called a swarm-bot, composed of a swarm of s-bots, mobile robots with the ability to connect to and to disconnect from each other. We discuss the challenges involved in controlling a swarm-bot and address the problem of synthesizing controllers for the swarm-bot using artiﬁcial evolution. Speciﬁcally, we study aggregation and coordinated motion of the swarm-bot using a physics-based simulation of the system. Experiments, using a simpliﬁed simulation model of the s-bots, show that evolution can discover simple but eﬀective controllers for both the aggregation and the coordinated motion of the swarm-bot. Analysis of the evolved controllers shows that they have properties of scalability, that is, they continue to be eﬀective for larger group sizes, and of generality, that is, they produce similar behaviors for conﬁgurations diﬀerent from those they were originally evolved for. The portability of the evolved controllers to real s-bots is tested using a detailed simulation model which has been validated against the real s-bots in a companion paper in this same special issue.},
  year = {2004},
  date = {2004-09-00 2004-09},
  journalAbbreviation = {Autonomous Robots},
  title = {Evolving Self-Organizing Behaviors for a Swarm-Bot},
}
@journalArticle{dorigoGuestEditorial2004,
  url = {http://link.springer.com/10.1023/B:AURO.0000034008.48988.2b},
  author = {Dorigo, Marco and Şahin, Erol},
  accessDate = {2025-02-04 21:42:39},
  key = {3KCSR9D7},
  volume = {17},
  publicationTitle = {Autonomous Robots},
  issue = {2/3},
  ISSN = {0929-5593},
  language = {en},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {111-113},
  DOI = {10.1023/B:AURO.0000034008.48988.2b},
  year = {2004},
  date = {2004-09-00 2004-09},
  journalAbbreviation = {Autonomous Robots},
  title = {Guest Editorial},
}
@conferencePaper{carpinUSARSimRobotSimulator2007,
  url = {http://ieeexplore.ieee.org/document/4209284/},
  proceedingsTitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
  author = {Carpin, Stefano and Lewis, Mike and Wang, Jijun and Balakirsky, Stephen and Scrapper, Chris},
  conferenceName = {2007 IEEE International Conference on Robotics and Automation},
  publisher = {IEEE},
  accessDate = {2025-02-04 23:33:50},
  ISBN = {978-1-4244-0602-9 978-1-4244-0601-2},
  shortTitle = {USARSim},
  extra = {ISSN: 1050-4729},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {1400-1405},
  date = {2007-04-00 2007-04},
  language = {en},
  DOI = {10.1109/ROBOT.2007.363180},
  abstractNote = {This paper presents USARSim, an open source high ﬁdelity robot simulator that can be used both for research and education. USARSim offers many characteristics that differentiates it from most existing simulators. Most notably, it constitutes the simulation engine used to run the Virtual Robots Competition within the Robocup initiative. We describe its general architecture, describe examples of utilization, and provide a comprehensive overview for those interested in robot simulations for education, research and competitions.},
  year = {2007},
  key = {BS56UZQR},
  place = {Rome, Italy},
  title = {USARSim: a robot simulator for research and education},
}
@preprint{dimmigSurveySimulatorsAerial2023,
  url = {http://arxiv.org/abs/2311.02296},
  archiveID = {arXiv:2311.02296},
  extra = {arXiv:2311.02296 [cs]},
  libraryCatalog = {arXiv.org},
  author = {Dimmig, Cora A. and Silano, Giuseppe and McGuire, Kimberly and Gabellieri, Chiara and Hönig, Wolfgang and Moore, Joseph and Kobilarov, Marin},
  accessDate = {2023-11-09 14:48:21},
  language = {en},
  abstractNote = {Uncrewed Aerial Vehicle (UAV) research faces challenges with safety, scalability, costs, and ecological impact when conducting hardware testing. High-ﬁdelity simulators offer a vital solution by replicating real-world conditions to enable the development and evaluation of novel perception and control algorithms. However, the large number of available simulators poses a signiﬁcant challenge for researchers to determine which simulator best suits their speciﬁc use-case, based on each simulator’s limitations and customization readiness. This paper analyzes existing UAV simulators and decision factors for their selection, aiming to enhance the efﬁciency and safety of research endeavors.},
  repository = {arXiv},
  year = {2023},
  key = {RHBGCFVW},
  date = {2023-11-03 2023-11-03},
  title = {Survey of Simulators for Aerial Robots},
}
@bookSection{lacheleSwarmSimXRealTimeSimulation2012,
  url = {http://link.springer.com/10.1007/978-3-642-34327-8_34},
  date = {2012-00-00 2012},
  publisher = {Springer Berlin Heidelberg},
  accessDate = {2025-02-05 00:00:33},
  ISBN = {978-3-642-34326-1 978-3-642-34327-8},
  volume = {7628},
  shortTitle = {SwarmSimX},
  extra = {Series Title: Lecture Notes in Computer Science
DOI: 10.1007/978-3-642-34327-8_34},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {375-387},
  author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Noda, Itsuki and Ando, Noriaki and Brugali, Davide and Kuffner, James J. and Lächele, Johannes and Franchi, Antonio and Bülthoff, Heinrich H. and Robuffo Giordano, Paolo},
  language = {en},
  abstractNote = {In this paper we present a novel simulation environment called SwarmSimX with the ability to simulate dozens of robots in a realistic 3D environment. The software architecture of SwarmSimX allows new robots, sensors, and other libraries to be loaded at runtime, extending the functionality of the simulation environment signiﬁcantly. In addition, SwarmSimX allows an easy exchange of the underlying libraries used for the visual and physical simulation to incorporate diﬀerent libraries (e.g., improved or future versions). A major feature is also the possibility to perform the whole simulation in real-time allowing for human-in-the-loop or hardware-in-the-loop scenarios. SwarmSimX has been already employed in several works presenting haptic shared control of multiple mobile robots (e.g., quadrotor UAVs). Additionally, we present here two validation tests showing the physical ﬁdelity and the real-time performance of SwarmSimX. For the tests we used NVIDIA R © PhysX© R and Ogre3D as physics and rendering libraries, respectively.},
  bookTitle = {Simulation, Modeling, and Programming for Autonomous Robots},
  year = {2012},
  key = {967BBGIU},
  place = {Berlin, Heidelberg},
  title = {SwarmSimX: Real-Time Simulation Environment for Multi-robot Systems},
}
@preprint{shahAirSimHighFidelityVisual2017,
  shortTitle = {AirSim},
  libraryCatalog = {arXiv.org},
  language = {en},
  DOI = {10.48550/arXiv.1705.05065},
  title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},
  author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
  date = {2017-07-18 2017-07-18},
  accessDate = {2025-02-25 21:30:50},
  repository = {arXiv},
  archiveID = {arXiv:1705.05065},
  year = {2017},
  extra = {arXiv:1705.05065 [cs]},
  key = {I2363D9V},
  url = {http://arxiv.org/abs/1705.05065},
  abstractNote = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by ﬁrst implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world ﬂights.},
}
@conferencePaper{pinciroliARGoSModularMultiengine2011,
  shortTitle = {ARGoS},
  key = {47TEXWB8},
  accessDate = {2025-02-25 22:15:08},
  DOI = {10.1109/IROS.2011.6094829},
  title = {ARGoS: A modular, multi-engine simulator for heterogeneous swarm robotics},
  author = {Pinciroli, Carlo and Trianni, Vito and O'Grady, Rehan and Pini, Giovanni and Brutschy, Arne and Brambilla, Manuele and Mathews, Nithin and Ferrante, Eliseo and Di Caro, Gianni and Ducatelle, Frederick and Stirling, Timothy and Gutierrez, Alvaro and Gambardella, Luca Maria and Dorigo, Marco},
  place = {San Francisco, CA},
  libraryCatalog = {DOI.org (Crossref)},
  conferenceName = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
  publisher = {IEEE},
  ISBN = {978-1-61284-456-5 978-1-61284-454-1 978-1-61284-455-8},
  year = {2011},
  date = {2011-01-09 01-09-2011},
  pages = {5027-5034},
  url = {http://ieeexplore.ieee.org/document/6094829/},
  proceedingsTitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
}
@preprint{kolveAI2THORInteractive3D2022,
  shortTitle = {AI2-THOR},
  language = {en},
  accessDate = {2025-02-25 21:56:35},
  DOI = {10.48550/arXiv.1712.05474},
  title = {AI2-THOR: An Interactive 3D Environment for Visual AI},
  archiveID = {arXiv:1712.05474},
  abstractNote = {We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.},
  date = {2022-08-26 2022-08-26},
  author = {Kolve, Eric and Mottaghi, Roozbeh and Han, Winson and VanderBilt, Eli and Weihs, Luca and Herrasti, Alvaro and Deitke, Matt and Ehsani, Kiana and Gordon, Daniel and Zhu, Yuke and Kembhavi, Aniruddha and Gupta, Abhinav and Farhadi, Ali},
  repository = {arXiv},
  year = {2022},
  key = {4E5T3KKU},
  libraryCatalog = {arXiv.org},
  url = {http://arxiv.org/abs/1712.05474},
  extra = {arXiv:1712.05474 [cs]},
}
@journalArticle{gilDevelopmentDeploymentNew2015,
  journalAbbreviation = {Comp Applic In Engineering},
  ISSN = {1061-3773, 1099-0542},
  language = {en},
  accessDate = {2025-02-25 21:59:50},
  DOI = {10.1002/cae.21615},
  title = {Development and deployment of a new robotics toolbox for education},
  rights = {http://onlinelibrary.wiley.com/termsAndConditions#vor},
  author = {Gil, Arturo and Reinoso, Oscar and Marin, Jose Maria and Paya, Luis and Ruiz, Javier},
  abstractNote = {ABSTRACT
            
              
              
                This paper presents a new toolbox focussed on the teaching of robotic manipulators. The library works under Matlab and has been designed to strengthen the theoretical concepts explained during the theory lectures. The educational approach is focussed on teaching the main concepts through developing math modeling and simulation. In order to do this, the toolbox aims at the fulfillment of a set of practical sessions that allow the students to test most of the concepts of an introductory course in robotic manipulators. In addition, the library possesses features that typically needed the usage of proprietary software, such as the visualization of a realistic 3D representation of commercial robotic arms and the programming of those arms in an industrial language. The practices include the concepts of direct and inverse kinematics, inverse and direct dynamics, path planning and robot programming. As a transversal practice, during the sessions, the student is asked to choose and integrate a new robotic arm in the library, proposing a particular solution to the direct and inverse kinematic problem, as well as the inclusion of other important parameters. The library has been deployed during the last year in bachelor and master studies and has received a nice acceptance. Finally, the library has been assessed in terms of usefulness, design and usage by means of a student survey. In addition, the surveys were designed to establish a relation between the student perception of the system, the time spent on the tool and their learning achievements. © 2014 Wiley Periodicals, Inc. Comput Appl Eng Educ 23:443–454, 2015; View this article online at
                wileyonlinelibrary.com/journal/cae
                ; DOI
                10.1002/cae.21615},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {443-454},
  date = {2015-05-00 05-2015},
  volume = {23},
  year = {2015},
  publicationTitle = {Computer Applications in Engineering Education},
  key = {8EBTSD2S},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cae.21615},
  issue = {3},
}
@conferencePaper{inproceedings,
  date = {2021-12-00 2021-12},
  year = {2021},
  title = {AVIS engine: a high-performance simulation software for autonomous vehicles},
  extra = {Citation Key: inproceedings},
  key = {CFQNW7UG},
  author = {Zarif, Amir Mohammad and Tayebzadeh, Seyed Mohammad Hossein and Zarif Shahsavan, Amirmahdi and Sadeghnejad, Soroush},
}
@conferencePaper{zarifAVISEngineHighperformance2021,
  date = {2021-12-00 2021-12},
  year = {2021},
  title = {AVIS engine: a high-performance simulation software for autonomous vehicles},
  key = {CFQNW7UG},
  author = {Zarif, Amir Mohammad and Tayebzadeh, Seyed Mohammad Hossein and Zarif Shahsavan, Amirmahdi and Sadeghnejad, Soroush},
}
@conferencePaper{pmlr-v78-dosovitskiy17a,
  extra = {Citation Key: pmlr-v78-dosovitskiy17a},
  abstractNote = {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform’s utility for autonomous driving research.},
  proceedingsTitle = {Proceedings of the 1st annual conference on robot learning},
  date = {2017-11-13 13–15 Nov 2017},
  series = {Proceedings of machine learning research},
  pages = {1–16},
  publisher = {PMLR},
  volume = {78},
  year = {2017},
  title = {CARLA: An open urban driving simulator},
  key = {TYVJ7VLM},
  url = {https://proceedings.mlr.press/v78/dosovitskiy17a.html},
  author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen and Levine, Sergey and Vanhoucke, Vincent and Goldberg, Ken},
}
@conferencePaper{rohmerVREPVersatileScalable2013,
  ISBN = {978-1-4673-6358-7 978-1-4673-6357-0},
  pages = {1321-1326},
  key = {XB4UABF3},
  DOI = {10.1109/IROS.2013.6696520},
  libraryCatalog = {DOI.org (Crossref)},
  title = {V-REP: A versatile and scalable robot simulation framework},
  author = {Rohmer, Eric and Singh, Surya P. N. and Freese, Marc},
  place = {Tokyo},
  date = {2013-11-00 11-2013},
  year = {2013},
  language = {en},
  url = {http://ieeexplore.ieee.org/document/6696520/},
  proceedingsTitle = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  accessDate = {2025-02-25 22:12:50},
  conferenceName = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2013)},
  publisher = {IEEE},
  shortTitle = {V-REP},
}
@journalArticle{millerGraspIt2004,
  publicationTitle = {IEEE Robotics & Automation Magazine},
  issue = {4},
  journalAbbreviation = {IEEE Robot. Automat. Mag.},
  language = {en},
  rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  DOI = {10.1109/MRA.2004.1371616},
  pages = {110-122},
  title = {GraspIt!},
  author = {Miller, A.T. and Allen, P.K.},
  date = {2004-12-00 12-2004},
  libraryCatalog = {DOI.org (Crossref)},
  ISSN = {1070-9932},
  year = {2004},
  accessDate = {2025-02-25 22:42:57},
  volume = {11},
  url = {http://ieeexplore.ieee.org/document/1371616/},
  key = {TJ3QXD4Z},
}
@conferencePaper{savvaHabitatPlatformEmbodied2019,
  key = {HD4DBEMU},
  abstractNote = {We present Habitat, a platform for research in embodied artiﬁcial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efﬁcient photorealistic 3D simulation. Speciﬁcally, Habitat consists of: (i) Habitat-Sim: a ﬂexible, high-performance 3D simulator with conﬁgurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast – when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU.},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {9338-9346},
  title = {Habitat: A Platform for Embodied AI Research},
  author = {Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and Parikh, Devi and Batra, Dhruv},
  proceedingsTitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  conferenceName = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher = {IEEE},
  date = {2019-10-00 10-2019},
  ISBN = {978-1-7281-4803-8},
  language = {en},
  DOI = {10.1109/ICCV.2019.00943},
  place = {Seoul, Korea (South)},
  year = {2019},
  accessDate = {2025-02-25 23:21:41},
  rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  url = {https://ieeexplore.ieee.org/document/9010745/},
  shortTitle = {Habitat},
}
@conferencePaper{echeverriaModularOpenRobots2011,
  key = {7AUIUNV5},
  abstractNote = {This paper presents MORSE, a new open–source robotics simulator. MORSE provides several features of interest to robotics projects: it relies on a component-based architecture to simulate sensors, actuators and robots; it is ﬂexible, able to specify simulations at variable levels of abstraction according to the systems being tested; it is capable of representing a large variety of heterogeneous robots and full 3D environments (aerial, ground, maritime); and it is designed to allow simulations of multiple robots systems. MORSE uses a “Softwarein-the-Loop” philosophy, i.e. it gives the possibility to evaluate the algorithms embedded in the software architecture of the robot within which they are to be integrated. Still, MORSE is independent of any robot architecture or communication framework (middleware).},
  DOI = {10.1109/ICRA.2011.5980252},
  pages = {46-51},
  title = {Modular open robots simulation engine: MORSE},
  author = {Echeverria, Gilberto and Lassabe, Nicolas and Degroote, Arnaud and Lemaignan, Severin},
  proceedingsTitle = {2011 IEEE International Conference on Robotics and Automation},
  libraryCatalog = {DOI.org (Crossref)},
  conferenceName = {2011 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  ISBN = {978-1-61284-386-5},
  date = {2011-05-00 05-2011},
  language = {en},
  place = {Shanghai, China},
  year = {2011},
  accessDate = {2025-02-25 23:33:20},
  url = {http://ieeexplore.ieee.org/document/5980252/},
  shortTitle = {Modular open robots simulation engine},
}
@document{coumans2021,
  extra = {Citation Key: coumans2021},
  date = {2016-00-00 2016–2021},
  year = {2016},
  title = {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
  key = {NJR6D9DD},
  url = {http://pybullet.org},
  author = {Coumans, Erwin and Bai, Yunfei},
}
@journalArticle{Baumgartner_PyBullet_Industrial_A_2023,
  extra = {Citation Key: Baumgartner_PyBullet_Industrial_A_2023},
  key = {HCCI73L6},
  date = {2023-05-00 2023-05},
  issue = {85},
  DOI = {10.21105/joss.05174},
  publicationTitle = {Journal of Open Source Software},
  pages = {5174},
  year = {2023},
  title = {PyBullet Industrial: A process-aware robot simulation},
  volume = {8},
  url = {https://joss.theoj.org/papers/10.21105/joss.05174},
  author = {Baumgärtner, Jan and Hansjosten, Malte and Schönhofen, Dominik and Fleischer, Prof. Dr.-Ing. Jürgen},
}
@conferencePaper{xiangSAPIENSimulAtedPartBased2020,
  key = {ZMUX8KZ9},
  abstractNote = {Building home assistant robots has long been a goal for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufﬁcient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simpliﬁcation and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set of articulated objects. SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We evaluate stateof-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that SAPIEN will open research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {11094-11104},
  title = {SAPIEN: A SimulAted Part-Based Interactive ENvironment},
  author = {Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu and Wang, He and Yi, Li and Chang, Angel X. and Guibas, Leonidas J. and Su, Hao},
  proceedingsTitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  conferenceName = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  date = {2020-06-00 6-2020},
  ISBN = {978-1-7281-7168-5},
  language = {en},
  DOI = {10.1109/CVPR42600.2020.01111},
  place = {Seattle, WA, USA},
  year = {2020},
  accessDate = {2025-02-25 23:50:48},
  rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  url = {https://ieeexplore.ieee.org/document/9156706/},
  shortTitle = {SAPIEN},
}
@bookSection{huguesSimbadAutonomousRobot2006,
  bookTitle = {From Animals to Animats 9},
  extra = {Series Title: Lecture Notes in Computer Science
DOI: 10.1007/11840541_68},
  accessDate = {2025-02-26 00:29:51},
  abstractNote = {Simbad is an open source Java 3d robot simulator for scientiﬁc and educational purposes. It is mainly dedicated to researchers and programmers who want a simple basis for studying Situated Artiﬁcial Intelligence, Machine Learning, and more generally AI algorithms, in the context of Autonomous Robotics and Autonomous Agents. It is is kept voluntarily readable and simple for fast implementation in the ﬁeld of Research and/or Education.},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {831-842},
  title = {Simbad: An Autonomous Robot Simulation Package for Education and Research},
  author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Nolfi, Stefano and Baldassarre, Gianluca and Calabretta, Raffaele and Hallam, John C. T. and Marocco, Davide and Meyer, Jean-Arcady and Miglino, Orazio and Parisi, Domenico and Hugues, Louis and Bredeche, Nicolas},
  place = {Berlin, Heidelberg},
  date = {2006-00-00 2006},
  ISBN = {978-3-540-38608-7 978-3-540-38615-5},
  language = {en},
  publisher = {Springer Berlin Heidelberg},
  year = {2006},
  key = {T5YGL7PP},
  volume = {4095},
  url = {http://link.springer.com/10.1007/11840541_68},
  shortTitle = {Simbad},
}
@journalArticle{Webots04,
  extra = {Citation Key: Webots04},
  key = {K9935FZ8},
  date = {2004-00-00 2004},
  issue = {1},
  publicationTitle = {Journal of Advanced Robotics Systems},
  pages = {39–42},
  year = {2004},
  title = {Webots: Professional mobile robot simulation},
  volume = {1},
  url = {http://www.ars-journal.com/International-Journal-of- Advanced-Robotic-Systems/Volume-1/39-42.pdf},
  author = {Michel, O.},
}
@computerProgram{genesisauthorsGenesisUniversalGenerative2024,
  date = {2024-12-00 2024-12},
  year = {2024},
  title = {Genesis: a universal and generative physics engine for robotics and beyond},
  key = {G2D35XHT},
  url = {https://github.com/Genesis-Embodied-AI/Genesis},
  author = {Genesis Authors, },
}
@journalArticle{blanco-claracoMultiVehicleSimulatorMVSim2023,
  extra = {arXiv:2302.11033 [cs]},
  key = {NT6EIY9Z},
  DOI = {10.1016/j.softx.2023.101443.},
  title = {MultiVehicle Simulator (MVSim): lightweight dynamics simulator for multiagents and mobile robotics research},
  author = {Blanco-Claraco, José-Luis and Tymchenko, Borys and Mañas-Alvarez, Francisco José and Cañadas-Aránega, Fernando and López-Gázquez, Ángel and Moreno, José Carlos},
  accessDate = {2025-02-26 00:52:22},
  volume = {23},
  year = {2023},
  publicationTitle = {SoftwareX},
  date = {2023-07-00 07-2023},
  language = {en},
  journalAbbreviation = {SoftwareX},
  abstractNote = {Development of applications related to closed-loop control requires either testing on the ﬁeld or on a realistic simulator, with the latter being more convenient, inexpensive, safe, and leading to shorter development cycles. To address that need, the present work introduces MVSim, a simulator for multiple vehicles or robots capable of running dozens of agents in simple scenarios, or a handful of them in complex scenarios. MVSim employs realistic physics-grounded friction models for tire-ground interaction, and aims at accurate and GPU-accelerated simulation of most common modern sensors employed in mobile robotics and autonomous vehicle research, such as depth and RGB cameras, or 2D and 3D LiDAR scanners. All depth-related sensors are able to accurately measure distances to 3D models provided by the user to deﬁne custom world elements. Eﬃcient simulation is achieved by means of focusing on ground vehicles, which allows the use of a simpliﬁed 2D physics engine for body collisions while solving wheel-ground interaction forces separately. The core parts of the system are written in C++ for maximum eﬃciency, while Python, ROS 1, and ROS 2 wrappers are also oﬀered for easy integration into user systems. A custom publish/subscribe protocol based on ZeroMQ (ZMQ) is deﬁned to allow for multiprocess applications to access or modify a running simulation. This simulator enables and makes easier to do research and development on vehicular dynamics, autonomous navigation algorithms, and simultaneous localization and mapping (SLAM) methods.},
  ISSN = {23527110},
  libraryCatalog = {arXiv.org},
  url = {http://arxiv.org/abs/2302.11033},
  pages = {101443},
  shortTitle = {MultiVehicle Simulator (MVSim)},
}
@conferencePaper{song2020flightmare,
  extra = {Citation Key: song2020flightmare},
  date = {2020-00-00 2020},
  year = {2020},
  title = {Flightmare: a flexible quadrotor simulator},
  key = {KYB8EGBZ},
  proceedingsTitle = {Conference on robot learning},
  author = {Song, Yunlong and Naji, Selim and Kaufmann, Elia and Loquercio, Antonio and Scaramuzza, Davide},
}
@conferencePaper{guerraFlightGogglesModularFramework2019,
  extra = {arXiv:1905.11377 [cs]},
  language = {en},
  DOI = {10.1109/IROS40897.2019.8968116},
  title = {FlightGoggles: A Modular Framework for Photorealistic Camera, Exteroceptive Sensor, and Dynamics Simulation},
  proceedingsTitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  author = {Guerra, Winter and Tal, Ezra and Murali, Varun and Ryou, Gilhyun and Karaman, Sertac},
  accessDate = {2025-02-26 01:05:58},
  date = {2019-11-00 11-2019},
  year = {2019},
  abstractNote = {FlightGoggles is a photorealistic sensor simulator for perception-driven robotic vehicles. The key contributions of FlightGoggles are twofold. First, FlightGoggles provides photorealistic exteroceptive sensor simulation using graphics assets generated with photogrammetry. Second, it provides the ability to combine (i) synthetic exteroceptive measurements generated in silico in real time and (ii) vehicle dynamics and proprioceptive measurements generated in motio by vehicle(s) in a motion-capture facility. FlightGoggles is capable of simulating a virtual-reality environment around autonomous vehicle(s). While a vehicle is in ﬂight in the FlightGoggles virtual reality environment, exteroceptive sensors are rendered synthetically in real time while all complex extrinsic dynamics are generated organically through the natural interactions of the vehicle. The FlightGoggles framework allows for researchers to accelerate development by circumventing the need to estimate complex and hard-to-model interactions such as aerodynamics, motor mechanics, battery electrochemistry, and behavior of other agents. The ability to perform vehicle-in-the-loop experiments with photorealistic exteroceptive sensor simulation facilitates novel research directions involving, e.g., fast and agile autonomous ﬂight in obstacle-rich environments, safe human interaction, and ﬂexible sensor selection. FlightGoggles has been utilized as the main test for selecting nine teams that will advance in the AlphaPilot autonomous drone racing challenge. We survey approaches and results from the top AlphaPilot teams, which may be of independent interest.},
  shortTitle = {FlightGoggles},
  libraryCatalog = {arXiv.org},
  url = {http://arxiv.org/abs/1905.11377},
  pages = {6941-6948},
  key = {IN4N43L9},
}
@document{drake,
  extra = {Citation Key: drake},
  date = {2019-00-00 2019},
  year = {2019},
  title = {Drake: Model-based design and verification for robotics},
  key = {YYJJNPEE},
  url = {https://drake.mit.edu},
  author = {Tedrake, Russ and Team, the Drake Development},
}
@inproceedings{todorovMuJoCoPhysicsEngine2012,
  title = {{{MuJoCo}}: {{A}} Physics Engine for Model-Based Control},
  shorttitle = {{{MuJoCo}}},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  date = {2012-10},
  pages = {5026--5033},
  publisher = {IEEE},
  location = {Vilamoura-Algarve, Portugal},
  doi = {10.1109/IROS.2012.6386109},
  url = {http://ieeexplore.ieee.org/document/6386109/},
  urldate = {2025-02-26},
  abstract = {We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efficient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difficulties with spring-dampers. Models are specified using either a high-level C++ API or an intuitive XML file format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-defined even in the presence of contacts and equality constraints. The model can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and finite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.},
  eventtitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2012)},
  isbn = {978-1-4673-1736-8 978-1-4673-1737-5 978-1-4673-1735-1},
  langid = {english}
}
@online{taoManiSkill3GPUParallelized2024,
  title = {{{ManiSkill3}}: {{GPU Parallelized Robotics Simulation}} and {{Rendering}} for {{Generalizable Embodied AI}}},
  shorttitle = {{{ManiSkill3}}},
  author = {Tao, Stone and Xiang, Fanbo and Shukla, Arth and Qin, Yuzhe and Hinrichsen, Xander and Yuan, Xiaodi and Bao, Chen and Lin, Xinsong and Liu, Yulin and Chan, Tse-kai and Gao, Yuan and Li, Xuanlin and Mu, Tongzhou and Xiao, Nan and Gurha, Arnav and Huang, Zhiao and Calandra, Roberto and Chen, Rui and Luo, Shan and Su, Hao},
  date = {2024-10-01},
  eprint = {2410.00425},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.00425},
  url = {http://arxiv.org/abs/2410.00425},
  urldate = {2025-03-16},
  abstract = {Simulation has enabled unprecedented compute-scalable approaches to robot learning. However, many existing simulation frameworks typically support a narrow range of scenes/tasks and lack features critical for scaling generalizable robotics and sim2real. We introduce and open source ManiSkill3, the fastest state-visual GPU parallelized robotics simulator with contact-rich physics targeting generalizable manipulation. ManiSkill3 supports GPU parallelization of many aspects including simulation+rendering, heterogeneous simulation, pointclouds/voxels visual input, and more. Simulation with rendering on ManiSkill3 can run 10-1000x faster with 2-3x less GPU memory usage than other platforms, achieving up to 30,000+ FPS in benchmarked environments due to minimal python/pytorch overhead in the system, simulation on the GPU, and the use of the SAPIEN parallel rendering system. Tasks that used to take hours to train can now take minutes. We further provide the most comprehensive range of GPU parallelized environments/tasks spanning 12 distinct domains including but not limited to mobile manipulation for tasks such as drawing, humanoids, and dextrous manipulation in realistic scenes designed by artists or real-world digital twins. In addition, millions of demonstration frames are provided from motion planning, RL, and teleoperation. ManiSkill3 also provides a comprehensive set of baselines that span popular RL and learning-from-demonstrations algorithms.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics}
}
@article{cattoIterativeDynamicsTemporal2005,
  title = {Iterative {{Dynamics}} with {{Temporal Coherence}}},
  author = {Catto, Erin and Park, Menlo},
  date = {2005},
  url = {https://api.semanticscholar.org/CorpusID:11084512},
  abstract = {This article introduces an iterative constraint solver for rigid body dynamics with contact. Our algorithm requires linear time and space and is easily expressed in vector form for fast execution on vector processors. The use of an iterative algorithm opens up the possibility for exploiting temporal coherence. A method for caching contact forces is presented that allows contact points to move from step to step and to appear and disappear. Examples are provided to illustrate the effectiveness of the algorithm.},
  langid = {english}
}
@thesis{diankovAutomatedConstructionRobotic2010,
  type = {phdthesis},
  title = {Automated Construction of Robotic Manipulation Programs},
  author = {Diankov, Rosen},
  date = {2010-08},
  number = {CMU-RI-TR-10-29},
  institution = {Carnegie Mellon University, Robotics Institute},
  url = {http://www.programmingvision.com/rosen_diankov_thesis.pdf}
}
@book{haykinNeuralNetworksLearning2009,
  title = {Neural Networks and Learning Machines},
  author = {Haykin, Simon S. and Haykin, Simon S.},
  date = {2009},
  edition = {3rd ed},
  publisher = {Prentice Hall},
  location = {New York},
  isbn = {978-0-13-147139-9},
  langid = {english},
  pagetotal = {906},
  keywords = {Adaptive filters,Neural networks (Computer science)},
  annotation = {OCLC: ocn237325326}
}
@book{ramonycajalHistologieSystemeNerveux1909,
  title = {Histologie Du Système Nerveux de l'homme \& Des Vertébrés},
  author = {Ramón Y Cajal, Santiago},
  date = {1909},
  publisher = {Maloine},
  location = {Paris},
  doi = {10.5962/bhl.title.48637},
  url = {https://www.biodiversitylibrary.org/bibliography/48637},
  urldate = {2025-04-06}
}
@book{calvettiIntroductionBayesianScientific2007,
  title = {Introduction to {{Bayesian Scientific Computing}}: {{Ten Lectures}} on {{Subjective Computing}}},
  shorttitle = {Introduction to {{Bayesian Scientific Computing}}},
  editor = {Calvetti, Daniela and Somersalo, Erkki},
  date = {2007},
  series = {Surveys and {{Tutorials}} in the {{Applied Mathematical Sciences}}},
  number = {2},
  publisher = {Springer Science+Business Media, LLC},
  location = {New York, NY},
  doi = {10.1007/978-0-387-73394-4},
  isbn = {978-0-387-73393-7 978-0-387-73394-4},
  langid = {english},
  pagetotal = {202}
}
@article{collinsReviewPhysicsSimulators2021,
  title = {A {{Review}} of {{Physics Simulators}} for {{Robotic Applications}}},
  author = {Collins, Jack and Chand, Shelvin and Vanderkop, Anthony and Howard, David},
  date = {2021},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {9},
  pages = {51416--51431},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3068769},
  url = {https://ieeexplore.ieee.org/document/9386154/},
  urldate = {2023-10-18},
  abstract = {The use of simulators in robotics research is widespread, underpinning the majority of recent advances in the field. There are now more options available to researchers than ever before, however navigating through the plethora of choices in search of the right simulator is often non-trivial. Depending on the field of research and the scenario to be simulated there will often be a range of suitable physics simulators from which it is difficult to ascertain the most relevant one. We have compiled a broad review of physics simulators for use within the major fields of robotics research. More specifically, we navigate through key sub-domains and discuss the features, benefits, applications and use-cases of the different simulators categorised by the respective research communities. Our review provides an extensive index of the leading physics simulators applicable to robotics researchers and aims to assist them in choosing the best simulator for their use case.},
  langid = {english}
}
@online{aldhaheriUnderwaterRoboticSimulators2025,
  title = {Underwater {{Robotic Simulators Review}} for {{Autonomous System Development}}},
  author = {Aldhaheri, Sara and Hu, Yang and Xie, Yongchang and Wu, Peng and Kanoulas, Dimitrios and Liu, Yuanchang},
  date = {2025-04-08},
  eprint = {2504.06245},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.06245},
  url = {http://arxiv.org/abs/2504.06245},
  urldate = {2025-04-09},
  abstract = {The increasing complexity of underwater robotic systems has led to a surge in simulation platforms designed to support perception, planning, and control tasks in marine environments. However, selecting the most appropriate underwater robotic simulator (URS) remains a challenge due to wide variations in fidelity, extensibility, and task suitability. This paper presents a comprehensive review and comparative analysis of five state-of-the-art, ROS-compatible, open-source URSs: Stonefish, DAVE, HoloOcean, MARUS, and UNav-Sim. Each simulator is evaluated across multiple criteria including sensor fidelity, environmental realism, sim-to-real capabilities, and research impact. We evaluate them across architectural design, sensor and physics modeling, task capabilities, and research impact. Additionally, we discuss ongoing challenges in sim-to-real transfer and highlight the need for standardization and benchmarking in the field. Our findings aim to guide practitioners in selecting effective simulation environments and inform future development of more robust and transferable URSs.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics}
}
@article{calderon-arceSwarmRoboticsSimulators2022,
  title = {Swarm Robotics: {{Simulators}}, Platforms and Applications Review},
  author = {Calderón-Arce, Cindy and Brenes-Torres, Juan Carlos and Solis-Ortega, Rebeca},
  date = {2022},
  journaltitle = {Computation},
  volume = {10},
  number = {6},
  issn = {2079-3197},
  doi = {10.3390/computation10060080},
  url = {https://www.mdpi.com/2079-3197/10/6/80},
  abstract = {This paper presents an updated and broad review of swarm robotics research papers regarding software, hardware, simulators and applications. The evolution from its concept to its real-life implementation is presented. Swarm robotics analysis is focused on four aspects: conceptualization, simulators, real-life robotics for swarm use, and applications. For simulators and robots, a detailed comparison between existing resources is made. A summary of the most used swarm robotics applications and behaviors is included.}
}
@article{tselegkaridisSimulatorsEducationalRobotics2021,
  title = {Simulators in {{Educational Robotics}}: {{A Review}}},
  shorttitle = {Simulators in {{Educational Robotics}}},
  author = {Tselegkaridis, Sokratis and Sapounidis, Theodosios},
  date = {2021-01-01},
  journaltitle = {Education Sciences},
  shortjournal = {Education Sciences},
  volume = {11},
  number = {1},
  pages = {11},
  issn = {2227-7102},
  doi = {10.3390/educsci11010011},
  url = {https://www.mdpi.com/2227-7102/11/1/11},
  urldate = {2023-10-18},
  abstract = {Educational robotics (ER) seems to have a positive effect on students and, in many cases, might help them to successfully assimilate knowledge and skills. Thus, this paper focuses on ER and carries out a literature review on educational robotics simulators with Graphical User Interfaces (GUIs). The review searches for relevant papers which were published in the period 2013–2020 and extracted the characteristics of the simulators used. The simulators that we describe in this article cover various robotic technologies, offering students an easy way to engage with virtual robots and robotics mechanisms, such as wheeled robots or drones. Using these simulators, students might cover their educational needs or prepare themselves for educational robotic competitions by working in as realistic as possible conditions without hardware restrictions. In many cases, simulators might reduce the required cost to obtain a robotic system and increase availability. Focusing on educational robotics simulators, this paper presents seventeen simulators emphasizing key features such as: user’s age, robot’s type and programming language, development platform, capabilities, and scope of the simulator.},
  langid = {english}
}
@article{chatzilygeroudisRobotDARTVersatileRobot2024,
  title = {{{RobotDART}}: A Versatile Robot Simulator for Robotics and Machine Learning Researchers},
  author = {Chatzilygeroudis, Konstantinos and Totsila, Dionis},
  date = {2024},
  abstract = {Robot simulation plays a pivotal role in robotics and machine learning research, offering a cost-effective and safe means to develop, validate, and benchmark algorithms in various scenarios. With the growing complexity of robotic systems and the increasing demand for data-driven approaches in machine learning, there is a pressing need for versatile and efficient robot simulators that cater to the diverse requirements of researchers. In response to this demand, we introduce RobotDART, a high-performance and versatile robot simulator designed to empower researchers in robotics and machine learning with a powerful and flexible simulation environment.},
  langid = {english}
}
@article{camargoSystematicLiteratureReview2021,
  title = {Systematic {{Literature Review}} of {{Realistic Simulators Applied}} in {{Educational Robotics Context}}},
  author = {Camargo, Caio and Gonçalves, José and Conde, Miguel Á. and Rodríguez-Sedano, Francisco J. and Costa, Paulo and García-Peñalvo, Francisco J.},
  date = {2021-06-11},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {21},
  number = {12},
  pages = {4031},
  issn = {1424-8220},
  doi = {10.3390/s21124031},
  url = {https://www.mdpi.com/1424-8220/21/12/4031},
  urldate = {2025-04-09},
  abstract = {This paper presents a systematic literature review (SLR) about realistic simulators that can be applied in an educational robotics context. These simulators must include the simulation of actuators and sensors, the ability to simulate robots and their environment. During this systematic review of the literature, 559 articles were extracted from six different databases using the Population, Intervention, Comparison, Outcomes, Context (PICOC) method. After the selection process, 50 selected articles were included in this review. Several simulators were found and their features were also analyzed. As a result of this process, four realistic simulators were applied in the review’s referred context for two main reasons. The first reason is that these simulators have high fidelity in the robots’ visual modeling due to the 3D rendering engines and the second reason is because they apply physics engines, allowing the robot’s interaction with the environment.},
  langid = {english}
}
@article{liuRolePhysicsBasedSimulators2021,
  title = {The {{Role}} of {{Physics-Based Simulators}} in {{Robotics}}},
  author = {Liu, C. Karen and Negrut, Dan},
  date = {2021-05-03},
  journaltitle = {Annual Review of Control, Robotics, and Autonomous Systems},
  shortjournal = {Annu. Rev. Control Robot. Auton. Syst.},
  volume = {4},
  number = {1},
  pages = {35--58},
  issn = {2573-5144, 2573-5144},
  doi = {10.1146/annurev-control-072220-093055},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-control-072220-093055},
  urldate = {2025-04-09},
  abstract = {Physics-based simulation provides an accelerated and safe avenue for developing, verifying, and testing robotic control algorithms and prototype designs. In the quest to leverage machine learning for developing AI-enabled robots, physics-based simulation can generate a wealth of labeled training data in a short amount of time. Physics-based simulation also creates an ideal proving ground for developing intelligent robots that can both learn from their mistakes and be verifiable. This article provides an overview of the use of simulation in robotics, emphasizing how robots (with sensing and actuation components), the environment they operate in, and the humans they interact with are simulated in practice. It concludes with an overview of existing tools for simulation in robotics and a short discussion of aspects that limit the role that simulation plays today in intelligent robot design.},
  langid = {english}
}
@article{choiUseSimulationRobotics2021,
  title = {On the Use of Simulation in Robotics: {{Opportunities}}, Challenges, and Suggestions for Moving Forward},
  shorttitle = {On the Use of Simulation in Robotics},
  author = {Choi, HeeSun and Crump, Cindy and Duriez, Christian and Elmquist, Asher and Hager, Gregory and Han, David and Hearl, Frank and Hodgins, Jessica and Jain, Abhinandan and Leve, Frederick and Li, Chen and Meier, Franziska and Negrut, Dan and Righetti, Ludovic and Rodriguez, Alberto and Tan, Jie and Trinkle, Jeff},
  date = {2021-01-05},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {118},
  number = {1},
  pages = {e1907856118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1907856118},
  url = {https://pnas.org/doi/full/10.1073/pnas.1907856118},
  urldate = {2025-04-09},
  abstract = {The last five years marked a surge in interest for and use of smart robots, which operate in dynamic and unstructured environments and might interact with humans. We posit that well-validated computer simulation can provide a virtual proving ground that in many cases is instrumental in understanding safely, faster, at lower costs, and more thoroughly how the robots of the future should be designed and controlled for safe operation and improved performance. Against this backdrop, we discuss how simulation can help in robotics, barriers that currently prevent its broad adoption, and potential steps that can eliminate some of these barriers. The points and recommendations made concern the following simulation-in-robotics aspects: simulation of the dynamics of the robot; simulation of the virtual world; simulation of the sensing of this virtual world; simulation of the interaction between the human and the robot; and, in less depth, simulation of the communication between robots. This Perspectives contribution summarizes the points of view that coalesced during a 2018 National Science Foundation/Department of Defense/National Institute for Standards and Technology workshop dedicated to the topic at hand. The meeting brought together participants from a range of organizations, disciplines, and application fields, with expertise at the intersection of robotics, machine learning, and physics-based simulation.},
  langid = {english}
}
@article{reckhausOverviewSimulationEmulation2010,
  title = {An {{Overview}} about {{Simulation}} and {{Emulation}} in {{Robotics}}},
  author = {Reckhaus, Michael and Hochgeschwender, Nico and Paulus, Jan and Kraetzschmar, Gerhard K},
  date = {2010},
  abstract = {Simulation is well established in robotics and nearly every robot project benefits from it. Thereby the understanding on what insights a simulation shall bring or how it shall assist the development is totally different in every project. The main reason is that simulation covers all aspects in the robot development process as for example learning and tuning control parameters or testing the composition of algorithms in a robot architecture. Further, the broad application field led to a large number of different simulation environments where, depending on the application, some are designed to simulate special purpose hardware and others aim to target a wide range of robotic systems. In this paper we present an overview on different use cases for simulation in robotics. Dependent on the use cases we propose a categorization for robot simulators/emulators and present some concrete environments.},
  langid = {english}
}
@inproceedings{Teixeira2015EducationalRS,
  title = {Educational Robotic Simulators: A Systematic Literature Review},
  author = {Teixeira, J. V. and family=Silva Hounsell, given=Marcelo, prefix=da, useprefix=true},
  date = {2015},
  url = {https://api.semanticscholar.org/CorpusID:113610382}
}
@book{tanenbaumComputerNetworks2021,
  title = {Computer Networks},
  author = {Tanenbaum, Andrew S. and Feamster, Nickolas and Wetherall, D.},
  date = {2021},
  edition = {Sixth edition. [Rental edition]},
  publisher = {Pearson},
  location = {Upper Saddle River},
  isbn = {978-0-13-676405-2},
  langid = {english},
  annotation = {OCLC: 1085945855}
}
@thesis{brendanw.sullivanEverythingYouAlways2013,
  type = {phdthesis},
  title = {Everything {{You Always Wanted To Know About Mathematics}}},
  author = {{Brendan W. Sullivan} and {Professor John Mackey}},
  date = {2013-05-10},
  institution = {Carnegie Mellon University Pittsburgh, PA},
  url = {https://www.math.cmu.edu/users/jmackey/summer/bws_book.pdf},
  langid = {english}
}
@book{jacodProbabilityEssentials2004,
  title = {Probability {{Essentials}}},
  author = {Jacod, Jean and Protter, Philip},
  date = {2004},
  series = {Universitext},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-55682-1},
  url = {http://link.springer.com/10.1007/978-3-642-55682-1},
  urldate = {2023-10-15},
  isbn = {978-3-540-43871-7 978-3-642-55682-1},
  langid = {english}
}
@book{grinsteadGrinsteadSnellsIntroduction2009,
  title = {Grinstead and {{Snell}}'s {{Introduction}} to {{Probability}}},
  author = {Grinstead, Charles M},
  date = {2009},
  publisher = {Orange Grove Texts Plus},
  isbn = {978-1-61610-046-9},
  langid = {english},
  annotation = {OCLC: 1468177993}
}
@book{jaynesProbabilityTheoryLogic2003,
  title = {Probability Theory: The Logic of Science},
  shorttitle = {Probability Theory},
  author = {Jaynes, E. T.},
  editor = {Bretthorst, G. Larry},
  date = {2003},
  publisher = {Cambridge University Press},
  location = {Cambridge, UK},
  abstract = {A comprehensive introduction to the role of probability theory in general scientific endeavour. This book provides an original interpretation of probability theory, showing the subject to be an extension of logic, and presenting new results and applications. Ideal for scientists working in any area involving inference from incomplete information},
  isbn = {978-0-511-06589-7},
  langid = {english},
  annotation = {OCLC: 57254076}
}
@book{rasmussenGaussianProcessesMachine2008,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  date = {2008},
  series = {Adaptive Computation and Machine Learning},
  edition = {3. print},
  publisher = {MIT Press},
  location = {Cambridge, Mass.},
  isbn = {978-0-262-18253-9},
  langid = {english},
  pagetotal = {248}
}
@book{walpoleProbabilityStatisticsEngineers2017,
  title = {Probability \& Statistics for Engineers \& Scientists: {{MyStatLab}} Update},
  shorttitle = {Probability \& Statistics for Engineers \& Scientists},
  author = {Walpole, Ronald E. and Myers, Raymond H. and Myers, Sharon L. and Ye, Keying},
  date = {2017},
  edition = {Ninth edition},
  publisher = {Pearson},
  location = {Boston},
  isbn = {978-1-292-16141-9},
  langid = {english},
  annotation = {OCLC: 1014366070}
}
@book{deisenrothMathematicsMachineLearning,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  langid = {english},
  pagetotal = {412}
}
@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  date = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  location = {New York},
  isbn = {978-0-387-31073-2},
  langid = {english}
}
@book{bishopDeepLearningFoundations2024,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  shorttitle = {Deep {{Learning}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  date = {2024},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-45468-4},
  url = {https://link.springer.com/10.1007/978-3-031-45468-4},
  urldate = {2023-12-18},
  isbn = {978-3-031-45467-7 978-3-031-45468-4},
  langid = {english}
}
@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  isbn = {978-0-262-03561-3},
  pagetotal = {775},
  keywords = {Machine learning}
}
@book{kolmogoroffGrundbegriffeWahrscheinlichkeitsrechnung1933,
  title = {Grundbegriffe der Wahrscheinlichkeitsrechnung},
  author = {Kolmogoroff, A.},
  date = {1933},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-49888-6},
  url = {http://link.springer.com/10.1007/978-3-642-49888-6},
  urldate = {2025-04-21},
  isbn = {978-3-642-49596-0 978-3-642-49888-6},
  langid = {ngerman}
}
@book{kolmogorovFoundationsTheoryProbability1960,
  title = {Foundations of the {{Theory}} of {{Probability}} ({{Translation}})},
  author = {Kolmogorov, A.},
  date = {1960},
  edition = {2},
  publisher = {Chelsea Publishing Co.},
  location = {New York},
  langid = {english}
}
@book{stiglerHistoryStatisticsMeasurement1986,
  title = {The History of Statistics: The Measurement of Uncertainty before 1900},
  shorttitle = {The History of Statistics},
  author = {Stigler, Stephen M.},
  date = {1986},
  publisher = {Belknap Press of Harvard University Press},
  location = {Cambridge, Mass},
  isbn = {978-0-674-40340-6},
  langid = {english},
  pagetotal = {410},
  keywords = {History,Statistics}
}
@book{taoIntroductionMeasureTheory2011,
  title = {An Introduction to Measure Theory},
  author = {Tao, Terence},
  date = {2011},
  series = {Graduate {{Studies}} in {{Mathematics}}},
  number = {126},
  publisher = {American Mathematical Society},
  location = {Providence, Rhode Island},
  isbn = {978-1-4704-6640-4 978-1-4704-1187-9},
  langid = {english},
  pagetotal = {1}
}
@book{durrettProbabilityTheoryExamples2019,
  title = {Probability: {{Theory}} and {{Examples}}},
  shorttitle = {Probability},
  author = {Durrett, Rick},
  date = {2019-04-18},
  edition = {5},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781108591034, Public version: https://services.math.duke.edu/~rtd/PTE/pte.html},
  url = {https://www.cambridge.org/core/product/identifier/9781108591034/type/book},
  urldate = {2025-04-21},
  isbn = {978-1-108-59103-4 978-1-108-47368-2}
}
@online{michaelbetancourtProbabilityTheoryScientists2018,
  title = {Probability {{Theory}} ({{For Scientists}} and {{Engineers}})},
  author = {{Michael Betancourt}},
  date = {2018-10},
  url = {https://betanalpha.github.io/assets/case_studies/probability_theory.html},
  urldate = {2025-04-23}
}
@book{follandRealAnalysisModern1999,
  title = {Real Analysis: Modern Techniques and Their Applications},
  shorttitle = {Real Analysis},
  author = {Folland, G. B.},
  date = {1999},
  series = {Pure and Applied Mathematics},
  edition = {2nd ed},
  publisher = {Wiley},
  location = {New York},
  isbn = {978-0-471-31716-6},
  langid = {english},
  pagetotal = {386},
  keywords = {Functions of real variables,Mathematical analysis}
}
@article{newburyReviewDifferentiableSimulators2024,
  title = {A {{Review}} of {{Differentiable Simulators}}},
  author = {Newbury, Rhys and Collins, Jack and He, Kerry and Pan, Jiahe and Posner, Ingmar and Howard, David and Cosgun, Akansel},
  date = {2024},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {12},
  pages = {97581--97604},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3425448},
  url = {https://ieeexplore.ieee.org/document/10589638/},
  urldate = {2025-04-27}
}
@article{nikolaievComparativeReviewDrone2024,
  title = {Comparative {{Review}} of {{Drone Simulators}}},
  author = {Nikolaiev, Mykola and Novotarskyi, Mykhailo},
  date = {2024-10-02},
  journaltitle = {Information, Computing and Intelligent systems},
  shortjournal = {Inf. Comput. and Intell. syst. j.},
  number = {4},
  pages = {79--98},
  issn = {2786-8729, 2708-4930},
  doi = {10.20535/2786-8729.4.2024.300614},
  url = {https://itvisnyk.kpi.ua/article/view/300614},
  urldate = {2025-04-27},
  abstract = {The rapid development of Unmanned Aerial Vehicles (UAVs), particularly drones, has revolutionized various sectors, including agriculture, mapping, search and rescue operations and more. There is an urgent need for simulation environments to develop algorithms for complex trajectory evolutions in tasks like package delivery and environmental monitoring, to avoid the significant risks associated with real-world testing. One of the primary challenges in UAV research is the diversity and fragmentation of available simulation tools, complicating the selection of appropriate simulators for specific practical tasks. Researchers must balance trade-offs such as simulation speed, the accuracy of physical law emulation, sensor integration, and user interface quality. The absence of a universal simulator that includes high-fidelity physics, comprehensive sensor modeling, and scalability for drone swarm simulations is a significant issue. Known UAV simulators have certain advantages and disadvantages, but none provide a comprehensive solution to meet all the requirements for modern research and development. Integrating various sensors, such as cameras, LiDAR, GPS, and IMUs, into simulation systems remains a technical challenge, limiting the applicability of existing simulators. Additionally, the availability and support infrastructure for effective simulators can vary significantly, impacting their adoption and sustainability. Therefore, the main problem is the lack of a universal simulator that meets the diverse and specific needs of UAV research and development. A standardized approach to UAV simulation could improve the comparability of research results, simplify selection efforts, and create a unified basis for evaluating simulator performance. Advances in aerodynamic modeling, especially for quadcopters and fixed-wing UAVs, could enhance simulation accuracy and realism, better supporting the development of advanced technologies. Future research aims to develop more comprehensive, high-fidelity, and scalable simulation environments. This involves integrating innovative sensor modeling approaches, improving swarm dynamics modeling, and enhancing user accessibility and support. Key areas for improvement include sensor integration to model a wide range of sensors, improving swarm dynamics simulation to effectively model complex behaviors and interactions among multiple drones, simplifying user interfaces, providing comprehensive documentation, ensuring robust community support, developing standardized criteria for comparing and evaluating different simulators, and incorporating detailed aerodynamic principles to enhance simulation accuracy. Addressing these issues in the development of UAV simulators is crucial for advancing aerial robotics. Developing simulation environments with integrated advanced sensor capabilities, improved swarm dynamics modeling, and user-friendly interfaces can enhance the effectiveness and efficiency of UAV development. Standardized evaluation criteria and detailed aerodynamic modeling will support the evolution of UAV technologies, ensuring safer, more reliable, and innovative applications across various sectors. These enhancements will foster innovation, technological progress, and operational efficiency in real-world conditions.}
}
@book{hammingArtProbabilityScientists1991,
  title = {The Art of Probability for Scientists and Engineers},
  author = {Hamming, Richard Wesley},
  date = {1991},
  publisher = {Addison-Wesley},
  location = {Redwood City (Calif.)},
  isbn = {978-0-201-51058-4},
  langid = {english}
}
@inproceedings{atapour-abarghoueiRealTimeMonocularDepth2018,
  title = {Real-{{Time Monocular Depth Estimation Using Synthetic Data}} with {{Domain Adaptation}} via {{Image Style Transfer}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Atapour-Abarghouei, Amir and Breckon, Toby P.},
  date = {2018-06},
  pages = {2800--2810},
  publisher = {IEEE},
  location = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00296},
  url = {https://ieeexplore.ieee.org/document/8578394/},
  urldate = {2025-01-20},
  abstract = {Monocular depth estimation using learning-based approaches has become promising in recent years. However, most monocular depth estimators either need to rely on large quantities of ground truth depth data, which is extremely expensive and difficult to obtain, or predict disparity as an intermediary step using a secondary supervisory signal leading to blurring and other artefacts. Training a depth estimation model using pixel-perfect synthetic data can resolve most of these issues but introduces the problem of domain bias. This is the inability to apply a model trained on synthetic data to real-world scenarios. With advances in image style transfer and its connections with domain adaptation (Maximum Mean Discrepancy), we take advantage of style transfer and adversarial training to predict pixel perfect depth from a single real-world color image based on training over a large corpus of synthetic environment data. Experimental results indicate the efficacy of our approach compared to contemporary state-of-the-art techniques.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  keywords = {real-time-mde}
}
