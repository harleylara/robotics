@preprint{bhoiMonocularDepthEstimation2019,
  extra = {arXiv:1901.09402 [cs]},
  author = {Bhoi, Amlaan},
  abstractNote = {Monocular depth estimation is often described as an illposed and inherently ambiguous problem. Estimating depth from 2D images is a crucial step in scene reconstruction, 3D object recognition, segmentation, and detection. The problem can be framed as: given a single RGB image as input, predict a dense depth map for each pixel. This problem is worsened by the fact that most scenes have large texture and structural variations, object occlusions, and rich geometric detailing. All these factors contribute to difﬁculty in accurate depth estimation. In this paper, we review ﬁve papers that attempt to solve the depth estimation problem with various techniques including supervised, weakly-supervised, and unsupervised learning techniques. We then compare these papers and understand the improvements made over one another. Finally, we explore potential improvements that can aid to better solve this problem.},
  key = {8TVFATMW},
  libraryCatalog = {arXiv.org},
  title = {Monocular Depth Estimation: A Survey},
  year = {2019},
  language = {en},
  url = {http://arxiv.org/abs/1901.09402},
  date = {2019-01-27 2019-01-27},
  shortTitle = {Monocular Depth Estimation},
  repository = {arXiv},
  archiveID = {arXiv:1901.09402},
  accessDate = {2024-05-28 15:00:24},
}
@journalArticle{padkanEVALUATINGMONOCULARDEPTH2023,
  publicationTitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  journalAbbreviation = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.},
  abstractNote = {Depth estimation from monocular images has become a prominent focus in photogrammetry and computer vision research. Monocular Depth Estimation (MDE), which involves determining depth from a single RGB image, offers numerous advantages, including applications in simultaneous localization and mapping (SLAM), scene comprehension, 3D modeling, robotics, and autonomous driving. Depth information retrieval becomes especially crucial in situations where other sources like stereo images, optical flow, or point clouds are not available. In contrast to traditional stereo or multi-view methods, MDE techniques require fewer computational resources and smaller datasets. This research work presents a comprehensive analysis and evaluation of some state-of-the-art MDE methods, considering their ability to infer depth information in terrestrial images. The evaluation includes quantitative assessments using ground truth data, including 3D analyses and inference time.},
  ISSN = {2194-9034},
  author = {Padkan, N. and Trybala, P. and Battisti, R. and Remondino, F. and Bergeret, C.},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {137-144},
  key = {7LEQVMK2},
  title = {EVALUATING MONOCULAR DEPTH ESTIMATION METHODS},
  year = {2023},
  language = {en},
  url = {https://isprs-archives.copernicus.org/articles/XLVIII-1-W3-2023/137/2023/},
  rights = {https://creativecommons.org/licenses/by/4.0/},
  accessDate = {2024-06-27 02:01:56},
  date = {2023-10-19 2023-10-19},
  volume = {XLVIII-1/W3-2023},
  DOI = {10.5194/isprs-archives-XLVIII-1-W3-2023-137-2023},
}
@preprint{alhashimHighQualityMonocular2019,
  language = {en},
  accessDate = {2024-10-02 04:07:58},
  title = {High Quality Monocular Depth Estimation via Transfer Learning},
  extra = {arXiv:1812.11941 [cs]},
  archiveID = {arXiv:1812.11941},
  year = {2019},
  date = {2019-03-10 2019-03-10},
  url = {http://arxiv.org/abs/1812.11941},
  author = {Alhashim, Ibraheem and Wonka, Peter},
  abstractNote = {Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction. Existing solutions for depth estimation often produce blurry approximations of low resolution. This paper presents a convolutional neural network for computing a high-resolution depth map given a single RGB image with the help of transfer learning. Following a standard encoder-decoder architecture, we leverage features extracted using high performing pre-trained networks when initializing our encoder along with augmentation and training strategies that lead to more accurate results. We show how, even for a very simple decoder, our method is able to achieve detailed high-resolution depth maps. Our network, with fewer parameters and training iterations, outperforms state-of-the-art on two datasets and also produces qualitatively better results that capture object boundaries more faithfully. Code and corresponding pre-trained weights are made publicly available1.},
  libraryCatalog = {arXiv.org},
  repository = {arXiv},
  key = {32BVAV9H},
}
@conferencePaper{BlochLaurent2015,
  title = {Informatics in the light of some Leibniz’s works},
  year = {2016},
  date = {2016-05-00 2016-05},
  author = {Bloch, Laurent},
  key = {3YCK8RDT},
  extra = {Citation Key: BlochLaurent2015},
}
@preprint{tzesGraphNeuralNetworks2022,
  title = {Graph Neural Networks for Multi-Robot Active Information Acquisition},
  abstractNote = {This paper addresses the Multi-Robot Active Information Acquisition (AIA) problem, where a team of mobile robots, communicating through an underlying graph, estimates a hidden state expressing a phenomenon of interest. Applications like target tracking, coverage and SLAM can be expressed in this framework. Existing approaches, though, are either not scalable, unable to handle dynamic phenomena or not robust to changes in the communication graph. To counter these shortcomings, we propose an Information-aware Graph Block Network (I-GBNet), an AIA adaptation of Graph Neural Networks, that aggregates information over the graph representation and provides sequential-decision making in a distributed manner. The I-GBNet, trained via imitation learning with a centralized sampling-based expert solver, exhibits permutation equivariance and time invariance, while harnessing the superior scalability, robustness and generalizability to previously unseen environments and robot conﬁgurations. Experiments on signiﬁcantly larger graphs and dimensionality of the hidden state and more complex environments than those seen in training validate the properties of the proposed architecture and its efﬁcacy in the application of localization and tracking of dynamic targets.},
  archiveID = {arXiv:2209.12091},
  key = {UETYTIWH},
  extra = {arXiv:2209.12091 [cs]},
  year = {2022},
  url = {http://arxiv.org/abs/2209.12091},
  repository = {arXiv},
  language = {en},
  accessDate = {2023-08-15 11:31:30},
  author = {Tzes, Mariliza and Bousias, Nikolaos and Chatzipantazis, Evangelos and Pappas, George J.},
  libraryCatalog = {arXiv.org},
  date = {2022-09-24 2022-09-24},
}
@preprint{bochkovskiiDepthProSharp2024,
  language = {en},
  accessDate = {2024-11-11 15:51:14},
  author = {Bochkovskii, Aleksei and Delaunoy, Amaël and Germain, Hugo and Santos, Marcel and Zhou, Yichao and Richter, Stephan R. and Koltun, Vladlen},
  repository = {arXiv},
  extra = {arXiv:2410.02073 [cs]},
  abstractNote = {We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro},
  title = {Depth Pro: Sharp Monocular Metric Depth in Less Than a Second},
  libraryCatalog = {arXiv.org},
  url = {http://arxiv.org/abs/2410.02073},
  shortTitle = {Depth Pro},
  key = {JJ5C6VKZ},
  date = {2024-10-02 2024-10-02},
  year = {2024},
  archiveID = {arXiv:2410.02073},
}
@preprint{eigenDepthMapPrediction2014,
  year = {2014},
  archiveID = {arXiv:1406.2283},
  title = {Depth Map Prediction from a Single Image using a Multi-Scale Deep Network},
  author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
  abstractNote = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence sufﬁces for estimation, ﬁnding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that reﬁnes this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
  libraryCatalog = {arXiv.org},
  extra = {arXiv:1406.2283 [cs]},
  key = {TCZ78FYN},
  repository = {arXiv},
  accessDate = {2023-08-10 12:19:39},
  language = {en},
  url = {http://arxiv.org/abs/1406.2283},
  date = {2014-06-09 2014-06-09},
}
@journalArticle{zhaoMonocularDepthEstimation2020,
  libraryCatalog = {arXiv.org},
  pages = {1612-1627},
  author = {Zhao, Chaoqiang and Sun, Qiyu and Zhang, Chongzhen and Tang, Yang and Qian, Feng},
  extra = {arXiv:2003.06620 [cs]},
  url = {http://arxiv.org/abs/2003.06620},
  publicationTitle = {Science China Technological Sciences},
  language = {en},
  abstractNote = {Depth information is important for autonomous systems to perceive environments and estimate their own state. Traditional depth estimation methods, like structure from motion and stereo vision matching, are built on feature correspondences of multiple viewpoints. Meanwhile, the predicted depth maps are sparse. Inferring depth information from a single image (monocular depth estimation) is an ill-posed problem. With the rapid development of deep neural networks, monocular depth estimation based on deep learning has been widely studied recently and achieved promising performance in accuracy. Meanwhile, dense depth maps are estimated from single images by deep neural networks in an end-to-end manner. In order to improve the accuracy of depth estimation, different kinds of network frameworks, loss functions and training strategies are proposed subsequently. Therefore, we survey the current monocular depth estimation methods based on deep learning in this review. Initially, we conclude several widely used datasets and evaluation indicators in deep learning-based depth estimation. Furthermore, we review some representative existing methods according to different training manners: supervised, unsupervised and semisupervised. Finally, we discuss the challenges and provide some ideas for future researches in monocular depth estimation.},
  accessDate = {2024-06-14 06:36:43},
  ISSN = {1674-7321, 1869-1900},
  volume = {63},
  journalAbbreviation = {Sci. China Technol. Sci.},
  key = {5IWGDN7U},
  date = {2020-09-00 09-2020},
  DOI = {10.1007/s11431-020-1582-8},
  issue = {9},
  title = {Monocular Depth Estimation Based On Deep Learning: An Overview},
  year = {2020},
  shortTitle = {Monocular Depth Estimation Based On Deep Learning},
}
@conferencePaper{vandijkHowNeuralNetworks2019,
  rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  author = {Van Dijk, Tom and De Croon, Guido},
  conferenceName = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  ISBN = {978-1-7281-4803-8},
  abstractNote = {Deep neural networks have lead to a breakthrough in depth estimation from single images. Recent work shows that the quality of these estimations is rapidly increasing. It is clear that neural networks can see depth in single images. However, to the best of our knowledge, no work currently exists that analyzes what these networks have learned.},
  url = {https://ieeexplore.ieee.org/document/9009532/},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {2183-2191},
  proceedingsTitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  language = {en},
  place = {Seoul, Korea (South)},
  date = {2019-10-00 2010-10-00},
  key = {XMZVKHSM},
  title = {How Do Neural Networks See Depth in Single Images?},
  publisher = {IEEE},
  DOI = {10.1109/ICCV.2019.00227},
  year = {2019},
  accessDate = {2025-01-04 22:19:24},
}
@journalArticle{khaldiOverviewSwarmRobotics2015,
  date = {2015-09-17 2015-09-17},
  publicationTitle = {International Journal of Computer Applications},
  issue = {2},
  journalAbbreviation = {IJCA},
  author = {Khaldi, Belkacem and Cherif, Foudil},
  language = {en},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {31-37},
  url = {http://www.ijcaonline.org/research/volume126/number2/khaldi-2015-ijca-906000.pdf},
  key = {N2A9WKS6},
  year = {2015},
  abstractNote = {As an emergent research area by which swarm intelligence is applied to multi-robot systems; swarm robotics (a very particular and peculiar sub-area of collective robotics) studies how to coordinate large groups of relatively simple robots through the use of local rules. It focuses on studying the design of large amount of relatively simple robots, their physical bodies and their controlling behaviors. Since its introduction in 2000, several successful experimentations had been realized, and till now more projects are under investigations. This paper seeks to give an overview of this domain research; for the aim to orientate the readers, especially those who are newly coming to this research field.},
  ISSN = {09758887},
  DOI = {10.5120/ijca2015906000},
  title = {An Overview of Swarm Robotics: Swarm Intelligence Applied to Multi-robotics},
  shortTitle = {An Overview of Swarm Robotics},
  accessDate = {2025-02-03 15:31:06},
  volume = {126},
}
@journalArticle{saxenaLearningDepthSingle,
  key = {J3QQAXIN},
  title = {Learning Depth from Single Monocular Images},
  year = {NA},
  abstractNote = {We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufﬁcient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.},
  author = {Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y},
  language = {en},
  libraryCatalog = {Zotero},
}
@journalArticle{tanSwarmRoboticsCollective2013,
  key = {GLFLXVWD},
  title = {Swarm Robotics: Collective Behavior Inspired by Nature},
  url = {https://www.hilarispublisher.com/open-access/swarm-robotics-collective-behavior-inspired-by-nature-jcsb.1000e106.pdf},
  DOI = {10.4172/jcsb.1000e106},
  volume = {6},
  year = {2013},
  ISSN = {0974-7230},
  publicationTitle = {Journal of Computer Science & Systems Biology},
  date = {2013-00-00 2013},
  author = {Tan, Ying},
  language = {en},
  libraryCatalog = {Zotero},
}
@journalArticle{sharkeySwarmRoboticsMinimalism2007,
  key = {7U4USFRL},
  title = {Swarm robotics and minimalism},
  DOI = {10.1080/09540090701584970},
  year = {2007},
  pages = {245-260},
  issue = {3},
  libraryCatalog = {DOI.org (Crossref)},
  date = {2007-09-00 2007-09},
  url = {https://www.tandfonline.com/doi/full/10.1080/09540090701584970},
  journalAbbreviation = {Connection Science},
  volume = {19},
  author = {Sharkey, Amanda J. C.},
  language = {en},
  ISSN = {0954-0091, 1360-0494},
  publicationTitle = {Connection Science},
  accessDate = {2025-02-04 02:13:30},
}
@bookSection{beniSwarmIntelligenceCellular1993,
  key = {22EJZYDL},
  title = {Swarm Intelligence in Cellular Robotic Systems},
  extra = {DOI: 10.1007/978-3-642-58069-7_38},
  place = {Berlin, Heidelberg},
  year = {1993},
  pages = {703-712},
  abstractNote = {Cellular Robotic Systems are capable of 'intelligent' behavior. The meaning of this intelligence is analyzed in the paper. We define robot intelligence and robot system intelligence in terms of unpredictability of improbable behavior. The concept of unpredictability is analyzed in relation to (1) statistical unpredictability, (2) inaccessibility, (3) undecidability, (4) intractability, and (5) non-representability. We argue that the latter two type of unpredictability, when exhibited by systems capable of producing order, can result in a non-trivial, different form of intelligent behavior (Swarm Intelligence). Engineering problems related to Swarm Intelligence are mentioned in relation to Cellular Robotic Systems which consist of collections of autonomous, non-synchronized, non-. intelligent robots cooperating to achieve global tasks.},
  publisher = {Springer Berlin Heidelberg},
  rights = {http://www.springer.com/tdm},
  libraryCatalog = {DOI.org (Crossref)},
  date = {1993-00-00 1993},
  url = {http://link.springer.com/10.1007/978-3-642-58069-7_38},
  bookTitle = {Robots and Biological Systems: Towards a New Bionics?},
  author = {Dario, Paolo and Sandini, Giulio and Aebischer, Patrick and Beni, Gerardo and Wang, Jing},
  ISBN = {978-3-642-63461-1 978-3-642-58069-7},
  language = {en},
  accessDate = {2025-02-04 02:24:35},
}
@thesis{ferranteControlArchitectureHeterogeneous2009,
  key = {YR4DAQKC},
  title = {A Control Architecture for a Heterogeneous Swarm of Robots},
  url = {https://iridia.ulb.ac.be/~mdorigo/HomePageDorigo/thesis/dea/FerranteMAS.pdf},
  year = {2009},
  thesisType = {PhD Thesis},
  date = {2009-00-00 2009},
  university = {UNIVERSIT ´E LIBRE DE BRUXELLES},
  abstractNote = {We propose a software architecture that can ease and speed up the development process of controllers for heterogeneous swarm systems. It is inherently modular and allows behaviors to be combined in layers and reused in multiple controllers. This can potentially speedup the development process of controllers since they become much more readable and well structured. We validated the architecture through an experiment of collective navigation and obstacle avoidance using two different types of robots. The experiments show that, using the proposed architecture, complex behaviors can indeed be built through the combination of very simple behaviors.},
  author = {Ferrante, Eliseo},
  language = {en},
  libraryCatalog = {Zotero},
}
@bookSection{yangSwarmBasedMetaheuristicAlgorithms2012,
  year = {2012},
  key = {XQTTDU6G},
  title = {Swarm-Based Metaheuristic Algorithms and No-Free-Lunch Theorems},
  url = {http://www.intechopen.com/books/theory-and-new-applications-of-swarm-intelligence/swarm-based-metaheuristic-algorithms-and-no-free-lunch-theorems},
  bookTitle = {Theory and New Applications of Swarm Intelligence},
  ISBN = {978-953-51-0364-6},
  publisher = {InTech},
  extra = {DOI: 10.5772/30852},
  date = {2012-03-16 2012-03-16},
  author = {Parpinelli, Rafael and Yang, Xin-She},
  language = {en},
  libraryCatalog = {DOI.org (Crossref)},
  accessDate = {2025-02-04 16:13:53},
}
@journalArticle{neshatArtificialFishSwarm2014,
  key = {KT57PTXC},
  title = {Artificial fish swarm algorithm: a survey of the state-of-the-art, hybridization, combinatorial and indicative applications},
  DOI = {10.1007/s10462-012-9342-2},
  year = {2014},
  pages = {965-997},
  abstractNote = {AFSA (artiﬁcial ﬁsh-swarm algorithm) is one of the best methods of optimization among the swarm intelligence algorithms. This algorithm is inspired by the collective movement of the ﬁsh and their various social behaviors. Based on a series of instinctive behaviors, the ﬁsh always try to maintain their colonies and accordingly demonstrate intelligent behaviors. Searching for food, immigration and dealing with dangers all happen in a social form and interactions between all ﬁsh in a group will result in an intelligent social behavior. This algorithm has many advantages including high convergence speed, ﬂexibility, fault tolerance and high accuracy. This paper is a review of AFSA algorithm and describes the evolution of this algorithm along with all improvements, its combination with various methods as well as its applications. There are many optimization methods which have a afﬁnity with this method and the result of this combination will improve the performance of this method. Its disadvantages include high time complexity, lack of balance between global and local search, in addition to lack of beneﬁting from the experiences of group members for the next movements.},
  issue = {4},
  rights = {http://www.springer.com/tdm},
  libraryCatalog = {DOI.org (Crossref)},
  date = {2014-12-00 2014-12},
  url = {http://link.springer.com/10.1007/s10462-012-9342-2},
  shortTitle = {Artificial fish swarm algorithm},
  journalAbbreviation = {Artif Intell Rev},
  publicationTitle = {Artificial Intelligence Review},
  author = {Neshat, Mehdi and Sepidnam, Ghodrat and Sargolzaei, Mehdi and Toosi, Adel Najaran},
  language = {en},
  ISSN = {0269-2821, 1573-7462},
  volume = {42},
  accessDate = {2025-02-04 16:17:54},
}
@journalArticle{hamdiBeebasedAlgorithmsReview2010,
  year = {2010},
  date = {2010-10-00 10-2010},
  language = {en},
  libraryCatalog = {Zotero},
  pages = {2},
  title = {Bee-based algorithms: a review},
  key = {UA542PLJ},
  author = {Hamdi, A and Monmarché, N and Alimi, M Adel and Slimane, M},
  url = {https://www.researchgate.net/profile/Amira-Hamdi/publication/237844606_META10_Bee-based_algorithms_a_review_META_2010/links/0046351be9feb2e247000000/META10-Bee-based-algorithms-a-review-META-2010.pdf},
}
@book{sahinSwarmRoboticsSAB2005,
  seriesNumber = {3342},
  author = {Şahin, Erol and Spears, William M.},
  series = {Lecture notes in computer science, State-of-the-art survey},
  publisher = {Springer},
  ISBN = {978-3-540-24296-3},
  shortTitle = {Swarm robotics},
  language = {en},
  libraryCatalog = {Library of Congress ISBN},
  year = {2005},
  numPages = {174},
  key = {HNUULXXY},
  extra = {Meeting Name: International Conference on Simulation of Adaptive Behavior
OCLC: ocm57597119},
  callNumber = {TJ210.3 .I633 2004},
  date = {2005-00-00 2005},
  place = {Berlin ; New York},
  title = {Swarm robotics: SAB 2004 international workshop, Santa Monica, CA, USA, July 17, 2004: revised selected papers},
}
@journalArticle{dorigoEvolvingSelfOrganizingBehaviors2004,
  url = {http://link.springer.com/10.1023/B:AURO.0000033973.24945.f3},
  author = {Dorigo, Marco and Trianni, Vito and Şahin, Erol and Groß, Roderich and Labella, Thomas H. and Baldassarre, Gianluca and Nolfi, Stefano and Deneubourg, Jean-Louis and Mondada, Francesco and Floreano, Dario and Gambardella, Luca M.},
  accessDate = {2025-02-04 20:55:06},
  key = {XDTAB9H8},
  volume = {17},
  publicationTitle = {Autonomous Robots},
  issue = {2/3},
  ISSN = {0929-5593},
  language = {en},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {223-245},
  DOI = {10.1023/B:AURO.0000033973.24945.f3},
  abstractNote = {In this paper, we introduce a self-assembling and self-organizing artifact, called a swarm-bot, composed of a swarm of s-bots, mobile robots with the ability to connect to and to disconnect from each other. We discuss the challenges involved in controlling a swarm-bot and address the problem of synthesizing controllers for the swarm-bot using artiﬁcial evolution. Speciﬁcally, we study aggregation and coordinated motion of the swarm-bot using a physics-based simulation of the system. Experiments, using a simpliﬁed simulation model of the s-bots, show that evolution can discover simple but eﬀective controllers for both the aggregation and the coordinated motion of the swarm-bot. Analysis of the evolved controllers shows that they have properties of scalability, that is, they continue to be eﬀective for larger group sizes, and of generality, that is, they produce similar behaviors for conﬁgurations diﬀerent from those they were originally evolved for. The portability of the evolved controllers to real s-bots is tested using a detailed simulation model which has been validated against the real s-bots in a companion paper in this same special issue.},
  year = {2004},
  date = {2004-09-00 2004-09},
  journalAbbreviation = {Autonomous Robots},
  title = {Evolving Self-Organizing Behaviors for a Swarm-Bot},
}
@journalArticle{dorigoGuestEditorial2004,
  url = {http://link.springer.com/10.1023/B:AURO.0000034008.48988.2b},
  author = {Dorigo, Marco and Şahin, Erol},
  accessDate = {2025-02-04 21:42:39},
  key = {3KCSR9D7},
  volume = {17},
  publicationTitle = {Autonomous Robots},
  issue = {2/3},
  ISSN = {0929-5593},
  language = {en},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {111-113},
  DOI = {10.1023/B:AURO.0000034008.48988.2b},
  year = {2004},
  date = {2004-09-00 2004-09},
  journalAbbreviation = {Autonomous Robots},
  title = {Guest Editorial},
}
@conferencePaper{carpinUSARSimRobotSimulator2007,
  url = {http://ieeexplore.ieee.org/document/4209284/},
  proceedingsTitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
  author = {Carpin, Stefano and Lewis, Mike and Wang, Jijun and Balakirsky, Stephen and Scrapper, Chris},
  conferenceName = {2007 IEEE International Conference on Robotics and Automation},
  publisher = {IEEE},
  accessDate = {2025-02-04 23:33:50},
  ISBN = {978-1-4244-0602-9 978-1-4244-0601-2},
  shortTitle = {USARSim},
  extra = {ISSN: 1050-4729},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {1400-1405},
  date = {2007-04-00 2007-04},
  language = {en},
  DOI = {10.1109/ROBOT.2007.363180},
  abstractNote = {This paper presents USARSim, an open source high ﬁdelity robot simulator that can be used both for research and education. USARSim offers many characteristics that differentiates it from most existing simulators. Most notably, it constitutes the simulation engine used to run the Virtual Robots Competition within the Robocup initiative. We describe its general architecture, describe examples of utilization, and provide a comprehensive overview for those interested in robot simulations for education, research and competitions.},
  year = {2007},
  key = {BS56UZQR},
  place = {Rome, Italy},
  title = {USARSim: a robot simulator for research and education},
}
@preprint{dimmigSurveySimulatorsAerial2023,
  url = {http://arxiv.org/abs/2311.02296},
  archiveID = {arXiv:2311.02296},
  extra = {arXiv:2311.02296 [cs]},
  libraryCatalog = {arXiv.org},
  author = {Dimmig, Cora A. and Silano, Giuseppe and McGuire, Kimberly and Gabellieri, Chiara and Hönig, Wolfgang and Moore, Joseph and Kobilarov, Marin},
  accessDate = {2023-11-09 14:48:21},
  language = {en},
  abstractNote = {Uncrewed Aerial Vehicle (UAV) research faces challenges with safety, scalability, costs, and ecological impact when conducting hardware testing. High-ﬁdelity simulators offer a vital solution by replicating real-world conditions to enable the development and evaluation of novel perception and control algorithms. However, the large number of available simulators poses a signiﬁcant challenge for researchers to determine which simulator best suits their speciﬁc use-case, based on each simulator’s limitations and customization readiness. This paper analyzes existing UAV simulators and decision factors for their selection, aiming to enhance the efﬁciency and safety of research endeavors.},
  repository = {arXiv},
  year = {2023},
  key = {RHBGCFVW},
  date = {2023-11-03 2023-11-03},
  title = {Survey of Simulators for Aerial Robots},
}
@bookSection{lacheleSwarmSimXRealTimeSimulation2012,
  url = {http://link.springer.com/10.1007/978-3-642-34327-8_34},
  date = {2012-00-00 2012},
  publisher = {Springer Berlin Heidelberg},
  accessDate = {2025-02-05 00:00:33},
  ISBN = {978-3-642-34326-1 978-3-642-34327-8},
  volume = {7628},
  shortTitle = {SwarmSimX},
  extra = {Series Title: Lecture Notes in Computer Science
DOI: 10.1007/978-3-642-34327-8_34},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {375-387},
  author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Noda, Itsuki and Ando, Noriaki and Brugali, Davide and Kuffner, James J. and Lächele, Johannes and Franchi, Antonio and Bülthoff, Heinrich H. and Robuffo Giordano, Paolo},
  language = {en},
  abstractNote = {In this paper we present a novel simulation environment called SwarmSimX with the ability to simulate dozens of robots in a realistic 3D environment. The software architecture of SwarmSimX allows new robots, sensors, and other libraries to be loaded at runtime, extending the functionality of the simulation environment signiﬁcantly. In addition, SwarmSimX allows an easy exchange of the underlying libraries used for the visual and physical simulation to incorporate diﬀerent libraries (e.g., improved or future versions). A major feature is also the possibility to perform the whole simulation in real-time allowing for human-in-the-loop or hardware-in-the-loop scenarios. SwarmSimX has been already employed in several works presenting haptic shared control of multiple mobile robots (e.g., quadrotor UAVs). Additionally, we present here two validation tests showing the physical ﬁdelity and the real-time performance of SwarmSimX. For the tests we used NVIDIA R © PhysX© R and Ogre3D as physics and rendering libraries, respectively.},
  bookTitle = {Simulation, Modeling, and Programming for Autonomous Robots},
  year = {2012},
  key = {967BBGIU},
  place = {Berlin, Heidelberg},
  title = {SwarmSimX: Real-Time Simulation Environment for Multi-robot Systems},
}
@preprint{shahAirSimHighFidelityVisual2017,
  shortTitle = {AirSim},
  libraryCatalog = {arXiv.org},
  language = {en},
  DOI = {10.48550/arXiv.1705.05065},
  title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},
  author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
  date = {2017-07-18 2017-07-18},
  accessDate = {2025-02-25 21:30:50},
  repository = {arXiv},
  archiveID = {arXiv:1705.05065},
  year = {2017},
  extra = {arXiv:1705.05065 [cs]},
  key = {I2363D9V},
  url = {http://arxiv.org/abs/1705.05065},
  abstractNote = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by ﬁrst implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world ﬂights.},
}
@conferencePaper{pinciroliARGoSModularMultiengine2011,
  shortTitle = {ARGoS},
  key = {47TEXWB8},
  accessDate = {2025-02-25 22:15:08},
  DOI = {10.1109/IROS.2011.6094829},
  title = {ARGoS: A modular, multi-engine simulator for heterogeneous swarm robotics},
  author = {Pinciroli, Carlo and Trianni, Vito and O'Grady, Rehan and Pini, Giovanni and Brutschy, Arne and Brambilla, Manuele and Mathews, Nithin and Ferrante, Eliseo and Di Caro, Gianni and Ducatelle, Frederick and Stirling, Timothy and Gutierrez, Alvaro and Gambardella, Luca Maria and Dorigo, Marco},
  place = {San Francisco, CA},
  libraryCatalog = {DOI.org (Crossref)},
  conferenceName = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
  publisher = {IEEE},
  ISBN = {978-1-61284-456-5 978-1-61284-454-1 978-1-61284-455-8},
  year = {2011},
  date = {2011-01-09 01-09-2011},
  pages = {5027-5034},
  url = {http://ieeexplore.ieee.org/document/6094829/},
  proceedingsTitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
}
@preprint{kolveAI2THORInteractive3D2022,
  shortTitle = {AI2-THOR},
  language = {en},
  accessDate = {2025-02-25 21:56:35},
  DOI = {10.48550/arXiv.1712.05474},
  title = {AI2-THOR: An Interactive 3D Environment for Visual AI},
  archiveID = {arXiv:1712.05474},
  abstractNote = {We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.},
  date = {2022-08-26 2022-08-26},
  author = {Kolve, Eric and Mottaghi, Roozbeh and Han, Winson and VanderBilt, Eli and Weihs, Luca and Herrasti, Alvaro and Deitke, Matt and Ehsani, Kiana and Gordon, Daniel and Zhu, Yuke and Kembhavi, Aniruddha and Gupta, Abhinav and Farhadi, Ali},
  repository = {arXiv},
  year = {2022},
  key = {4E5T3KKU},
  libraryCatalog = {arXiv.org},
  url = {http://arxiv.org/abs/1712.05474},
  extra = {arXiv:1712.05474 [cs]},
}
@journalArticle{gilDevelopmentDeploymentNew2015,
  journalAbbreviation = {Comp Applic In Engineering},
  ISSN = {1061-3773, 1099-0542},
  language = {en},
  accessDate = {2025-02-25 21:59:50},
  DOI = {10.1002/cae.21615},
  title = {Development and deployment of a new robotics toolbox for education},
  rights = {http://onlinelibrary.wiley.com/termsAndConditions#vor},
  author = {Gil, Arturo and Reinoso, Oscar and Marin, Jose Maria and Paya, Luis and Ruiz, Javier},
  abstractNote = {ABSTRACT
            
              
              
                This paper presents a new toolbox focussed on the teaching of robotic manipulators. The library works under Matlab and has been designed to strengthen the theoretical concepts explained during the theory lectures. The educational approach is focussed on teaching the main concepts through developing math modeling and simulation. In order to do this, the toolbox aims at the fulfillment of a set of practical sessions that allow the students to test most of the concepts of an introductory course in robotic manipulators. In addition, the library possesses features that typically needed the usage of proprietary software, such as the visualization of a realistic 3D representation of commercial robotic arms and the programming of those arms in an industrial language. The practices include the concepts of direct and inverse kinematics, inverse and direct dynamics, path planning and robot programming. As a transversal practice, during the sessions, the student is asked to choose and integrate a new robotic arm in the library, proposing a particular solution to the direct and inverse kinematic problem, as well as the inclusion of other important parameters. The library has been deployed during the last year in bachelor and master studies and has received a nice acceptance. Finally, the library has been assessed in terms of usefulness, design and usage by means of a student survey. In addition, the surveys were designed to establish a relation between the student perception of the system, the time spent on the tool and their learning achievements. © 2014 Wiley Periodicals, Inc. Comput Appl Eng Educ 23:443–454, 2015; View this article online at
                wileyonlinelibrary.com/journal/cae
                ; DOI
                10.1002/cae.21615},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {443-454},
  date = {2015-05-00 05-2015},
  volume = {23},
  year = {2015},
  publicationTitle = {Computer Applications in Engineering Education},
  key = {8EBTSD2S},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cae.21615},
  issue = {3},
}
@conferencePaper{inproceedings,
  date = {2021-12-00 2021-12},
  year = {2021},
  title = {AVIS engine: a high-performance simulation software for autonomous vehicles},
  extra = {Citation Key: inproceedings},
  key = {CFQNW7UG},
  author = {Zarif, Amir Mohammad and Tayebzadeh, Seyed Mohammad Hossein and Zarif Shahsavan, Amirmahdi and Sadeghnejad, Soroush},
}
@conferencePaper{zarifAVISEngineHighperformance2021,
  date = {2021-12-00 2021-12},
  year = {2021},
  title = {AVIS engine: a high-performance simulation software for autonomous vehicles},
  key = {CFQNW7UG},
  author = {Zarif, Amir Mohammad and Tayebzadeh, Seyed Mohammad Hossein and Zarif Shahsavan, Amirmahdi and Sadeghnejad, Soroush},
}
@conferencePaper{pmlr-v78-dosovitskiy17a,
  extra = {Citation Key: pmlr-v78-dosovitskiy17a},
  abstractNote = {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform’s utility for autonomous driving research.},
  proceedingsTitle = {Proceedings of the 1st annual conference on robot learning},
  date = {2017-11-13 13–15 Nov 2017},
  series = {Proceedings of machine learning research},
  pages = {1–16},
  publisher = {PMLR},
  volume = {78},
  year = {2017},
  title = {CARLA: An open urban driving simulator},
  key = {TYVJ7VLM},
  url = {https://proceedings.mlr.press/v78/dosovitskiy17a.html},
  author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen and Levine, Sergey and Vanhoucke, Vincent and Goldberg, Ken},
}
@conferencePaper{rohmerVREPVersatileScalable2013,
  ISBN = {978-1-4673-6358-7 978-1-4673-6357-0},
  pages = {1321-1326},
  key = {XB4UABF3},
  DOI = {10.1109/IROS.2013.6696520},
  libraryCatalog = {DOI.org (Crossref)},
  title = {V-REP: A versatile and scalable robot simulation framework},
  author = {Rohmer, Eric and Singh, Surya P. N. and Freese, Marc},
  place = {Tokyo},
  date = {2013-11-00 11-2013},
  year = {2013},
  language = {en},
  url = {http://ieeexplore.ieee.org/document/6696520/},
  proceedingsTitle = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  accessDate = {2025-02-25 22:12:50},
  conferenceName = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2013)},
  publisher = {IEEE},
  shortTitle = {V-REP},
}
@journalArticle{millerGraspIt2004,
  publicationTitle = {IEEE Robotics & Automation Magazine},
  issue = {4},
  journalAbbreviation = {IEEE Robot. Automat. Mag.},
  language = {en},
  rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  DOI = {10.1109/MRA.2004.1371616},
  pages = {110-122},
  title = {GraspIt!},
  author = {Miller, A.T. and Allen, P.K.},
  date = {2004-12-00 12-2004},
  libraryCatalog = {DOI.org (Crossref)},
  ISSN = {1070-9932},
  year = {2004},
  accessDate = {2025-02-25 22:42:57},
  volume = {11},
  url = {http://ieeexplore.ieee.org/document/1371616/},
  key = {TJ3QXD4Z},
}
@conferencePaper{savvaHabitatPlatformEmbodied2019,
  key = {HD4DBEMU},
  abstractNote = {We present Habitat, a platform for research in embodied artiﬁcial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efﬁcient photorealistic 3D simulation. Speciﬁcally, Habitat consists of: (i) Habitat-Sim: a ﬂexible, high-performance 3D simulator with conﬁgurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast – when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU.},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {9338-9346},
  title = {Habitat: A Platform for Embodied AI Research},
  author = {Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and Parikh, Devi and Batra, Dhruv},
  proceedingsTitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  conferenceName = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher = {IEEE},
  date = {2019-10-00 10-2019},
  ISBN = {978-1-7281-4803-8},
  language = {en},
  DOI = {10.1109/ICCV.2019.00943},
  place = {Seoul, Korea (South)},
  year = {2019},
  accessDate = {2025-02-25 23:21:41},
  rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  url = {https://ieeexplore.ieee.org/document/9010745/},
  shortTitle = {Habitat},
}
@conferencePaper{echeverriaModularOpenRobots2011,
  key = {7AUIUNV5},
  abstractNote = {This paper presents MORSE, a new open–source robotics simulator. MORSE provides several features of interest to robotics projects: it relies on a component-based architecture to simulate sensors, actuators and robots; it is ﬂexible, able to specify simulations at variable levels of abstraction according to the systems being tested; it is capable of representing a large variety of heterogeneous robots and full 3D environments (aerial, ground, maritime); and it is designed to allow simulations of multiple robots systems. MORSE uses a “Softwarein-the-Loop” philosophy, i.e. it gives the possibility to evaluate the algorithms embedded in the software architecture of the robot within which they are to be integrated. Still, MORSE is independent of any robot architecture or communication framework (middleware).},
  DOI = {10.1109/ICRA.2011.5980252},
  pages = {46-51},
  title = {Modular open robots simulation engine: MORSE},
  author = {Echeverria, Gilberto and Lassabe, Nicolas and Degroote, Arnaud and Lemaignan, Severin},
  proceedingsTitle = {2011 IEEE International Conference on Robotics and Automation},
  libraryCatalog = {DOI.org (Crossref)},
  conferenceName = {2011 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  ISBN = {978-1-61284-386-5},
  date = {2011-05-00 05-2011},
  language = {en},
  place = {Shanghai, China},
  year = {2011},
  accessDate = {2025-02-25 23:33:20},
  url = {http://ieeexplore.ieee.org/document/5980252/},
  shortTitle = {Modular open robots simulation engine},
}
@document{coumans2021,
  extra = {Citation Key: coumans2021},
  date = {2016-00-00 2016–2021},
  year = {2016},
  title = {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
  key = {NJR6D9DD},
  url = {http://pybullet.org},
  author = {Coumans, Erwin and Bai, Yunfei},
}
@journalArticle{Baumgartner_PyBullet_Industrial_A_2023,
  extra = {Citation Key: Baumgartner_PyBullet_Industrial_A_2023},
  key = {HCCI73L6},
  date = {2023-05-00 2023-05},
  issue = {85},
  DOI = {10.21105/joss.05174},
  publicationTitle = {Journal of Open Source Software},
  pages = {5174},
  year = {2023},
  title = {PyBullet Industrial: A process-aware robot simulation},
  volume = {8},
  url = {https://joss.theoj.org/papers/10.21105/joss.05174},
  author = {Baumgärtner, Jan and Hansjosten, Malte and Schönhofen, Dominik and Fleischer, Prof. Dr.-Ing. Jürgen},
}
@conferencePaper{xiangSAPIENSimulAtedPartBased2020,
  key = {ZMUX8KZ9},
  abstractNote = {Building home assistant robots has long been a goal for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufﬁcient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simpliﬁcation and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set of articulated objects. SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We evaluate stateof-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that SAPIEN will open research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {11094-11104},
  title = {SAPIEN: A SimulAted Part-Based Interactive ENvironment},
  author = {Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu and Wang, He and Yi, Li and Chang, Angel X. and Guibas, Leonidas J. and Su, Hao},
  proceedingsTitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  conferenceName = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  date = {2020-06-00 6-2020},
  ISBN = {978-1-7281-7168-5},
  language = {en},
  DOI = {10.1109/CVPR42600.2020.01111},
  place = {Seattle, WA, USA},
  year = {2020},
  accessDate = {2025-02-25 23:50:48},
  rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  url = {https://ieeexplore.ieee.org/document/9156706/},
  shortTitle = {SAPIEN},
}
@bookSection{huguesSimbadAutonomousRobot2006,
  bookTitle = {From Animals to Animats 9},
  extra = {Series Title: Lecture Notes in Computer Science
DOI: 10.1007/11840541_68},
  accessDate = {2025-02-26 00:29:51},
  abstractNote = {Simbad is an open source Java 3d robot simulator for scientiﬁc and educational purposes. It is mainly dedicated to researchers and programmers who want a simple basis for studying Situated Artiﬁcial Intelligence, Machine Learning, and more generally AI algorithms, in the context of Autonomous Robotics and Autonomous Agents. It is is kept voluntarily readable and simple for fast implementation in the ﬁeld of Research and/or Education.},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {831-842},
  title = {Simbad: An Autonomous Robot Simulation Package for Education and Research},
  author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Nolfi, Stefano and Baldassarre, Gianluca and Calabretta, Raffaele and Hallam, John C. T. and Marocco, Davide and Meyer, Jean-Arcady and Miglino, Orazio and Parisi, Domenico and Hugues, Louis and Bredeche, Nicolas},
  place = {Berlin, Heidelberg},
  date = {2006-00-00 2006},
  ISBN = {978-3-540-38608-7 978-3-540-38615-5},
  language = {en},
  publisher = {Springer Berlin Heidelberg},
  year = {2006},
  key = {T5YGL7PP},
  volume = {4095},
  url = {http://link.springer.com/10.1007/11840541_68},
  shortTitle = {Simbad},
}
@journalArticle{Webots04,
  extra = {Citation Key: Webots04},
  key = {K9935FZ8},
  date = {2004-00-00 2004},
  issue = {1},
  publicationTitle = {Journal of Advanced Robotics Systems},
  pages = {39–42},
  year = {2004},
  title = {Webots: Professional mobile robot simulation},
  volume = {1},
  url = {http://www.ars-journal.com/International-Journal-of- Advanced-Robotic-Systems/Volume-1/39-42.pdf},
  author = {Michel, O.},
}
@computerProgram{genesisauthorsGenesisUniversalGenerative2024,
  date = {2024-12-00 2024-12},
  year = {2024},
  title = {Genesis: a universal and generative physics engine for robotics and beyond},
  key = {G2D35XHT},
  url = {https://github.com/Genesis-Embodied-AI/Genesis},
  author = {Genesis Authors, },
}
@journalArticle{blanco-claracoMultiVehicleSimulatorMVSim2023,
  extra = {arXiv:2302.11033 [cs]},
  key = {NT6EIY9Z},
  DOI = {10.1016/j.softx.2023.101443.},
  title = {MultiVehicle Simulator (MVSim): lightweight dynamics simulator for multiagents and mobile robotics research},
  author = {Blanco-Claraco, José-Luis and Tymchenko, Borys and Mañas-Alvarez, Francisco José and Cañadas-Aránega, Fernando and López-Gázquez, Ángel and Moreno, José Carlos},
  accessDate = {2025-02-26 00:52:22},
  volume = {23},
  year = {2023},
  publicationTitle = {SoftwareX},
  date = {2023-07-00 07-2023},
  language = {en},
  journalAbbreviation = {SoftwareX},
  abstractNote = {Development of applications related to closed-loop control requires either testing on the ﬁeld or on a realistic simulator, with the latter being more convenient, inexpensive, safe, and leading to shorter development cycles. To address that need, the present work introduces MVSim, a simulator for multiple vehicles or robots capable of running dozens of agents in simple scenarios, or a handful of them in complex scenarios. MVSim employs realistic physics-grounded friction models for tire-ground interaction, and aims at accurate and GPU-accelerated simulation of most common modern sensors employed in mobile robotics and autonomous vehicle research, such as depth and RGB cameras, or 2D and 3D LiDAR scanners. All depth-related sensors are able to accurately measure distances to 3D models provided by the user to deﬁne custom world elements. Eﬃcient simulation is achieved by means of focusing on ground vehicles, which allows the use of a simpliﬁed 2D physics engine for body collisions while solving wheel-ground interaction forces separately. The core parts of the system are written in C++ for maximum eﬃciency, while Python, ROS 1, and ROS 2 wrappers are also oﬀered for easy integration into user systems. A custom publish/subscribe protocol based on ZeroMQ (ZMQ) is deﬁned to allow for multiprocess applications to access or modify a running simulation. This simulator enables and makes easier to do research and development on vehicular dynamics, autonomous navigation algorithms, and simultaneous localization and mapping (SLAM) methods.},
  ISSN = {23527110},
  libraryCatalog = {arXiv.org},
  url = {http://arxiv.org/abs/2302.11033},
  pages = {101443},
  shortTitle = {MultiVehicle Simulator (MVSim)},
}
@conferencePaper{song2020flightmare,
  extra = {Citation Key: song2020flightmare},
  date = {2020-00-00 2020},
  year = {2020},
  title = {Flightmare: a flexible quadrotor simulator},
  key = {KYB8EGBZ},
  proceedingsTitle = {Conference on robot learning},
  author = {Song, Yunlong and Naji, Selim and Kaufmann, Elia and Loquercio, Antonio and Scaramuzza, Davide},
}
@conferencePaper{guerraFlightGogglesModularFramework2019,
  extra = {arXiv:1905.11377 [cs]},
  language = {en},
  DOI = {10.1109/IROS40897.2019.8968116},
  title = {FlightGoggles: A Modular Framework for Photorealistic Camera, Exteroceptive Sensor, and Dynamics Simulation},
  proceedingsTitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  author = {Guerra, Winter and Tal, Ezra and Murali, Varun and Ryou, Gilhyun and Karaman, Sertac},
  accessDate = {2025-02-26 01:05:58},
  date = {2019-11-00 11-2019},
  year = {2019},
  abstractNote = {FlightGoggles is a photorealistic sensor simulator for perception-driven robotic vehicles. The key contributions of FlightGoggles are twofold. First, FlightGoggles provides photorealistic exteroceptive sensor simulation using graphics assets generated with photogrammetry. Second, it provides the ability to combine (i) synthetic exteroceptive measurements generated in silico in real time and (ii) vehicle dynamics and proprioceptive measurements generated in motio by vehicle(s) in a motion-capture facility. FlightGoggles is capable of simulating a virtual-reality environment around autonomous vehicle(s). While a vehicle is in ﬂight in the FlightGoggles virtual reality environment, exteroceptive sensors are rendered synthetically in real time while all complex extrinsic dynamics are generated organically through the natural interactions of the vehicle. The FlightGoggles framework allows for researchers to accelerate development by circumventing the need to estimate complex and hard-to-model interactions such as aerodynamics, motor mechanics, battery electrochemistry, and behavior of other agents. The ability to perform vehicle-in-the-loop experiments with photorealistic exteroceptive sensor simulation facilitates novel research directions involving, e.g., fast and agile autonomous ﬂight in obstacle-rich environments, safe human interaction, and ﬂexible sensor selection. FlightGoggles has been utilized as the main test for selecting nine teams that will advance in the AlphaPilot autonomous drone racing challenge. We survey approaches and results from the top AlphaPilot teams, which may be of independent interest.},
  shortTitle = {FlightGoggles},
  libraryCatalog = {arXiv.org},
  url = {http://arxiv.org/abs/1905.11377},
  pages = {6941-6948},
  key = {IN4N43L9},
}
@document{drake,
  extra = {Citation Key: drake},
  date = {2019-00-00 2019},
  year = {2019},
  title = {Drake: Model-based design and verification for robotics},
  key = {YYJJNPEE},
  url = {https://drake.mit.edu},
  author = {Tedrake, Russ and Team, the Drake Development},
}
